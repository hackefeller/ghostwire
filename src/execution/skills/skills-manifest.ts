// AUTO-GENERATED FILE - DO NOT EDIT
// Generated by src/script/build-skills-manifest.ts
// Run 'bun run src/script/build-skills-manifest.ts' to regenerate

import type { Skill } from "./types";

export const SKILLS_MANIFEST: ReadonlyArray<Skill> = [
  {
    "name": "agent-browser",
    "description": "Automates browser interactions for web testing, form filling, screenshots, and data extraction. Use when the user needs to navigate websites, interact with web pages, fill forms, take screenshots, test web applications, or extract information from web pages.",
    "template": "# Browser Automation with agent-browser\n\n## Quick start\n\n```bash\nagent-browser open <url>        # Navigate to page\nagent-browser snapshot -i       # Get interactive elements with refs\nagent-browser click @e1         # Click element by ref\nagent-browser fill @e2 \"text\"   # Fill input by ref\nagent-browser close             # Close browser\n```\n\n## Core workflow\n\n1. Navigate: `agent-browser open <url>`\n2. Snapshot: `agent-browser snapshot -i` (returns elements with refs like `@e1`, `@e2`)\n3. Interact using refs from the snapshot\n4. Re-snapshot after navigation or significant DOM changes\n\n## Commands\n\n### Navigation\n\n```bash\nagent-browser open <url>      # Navigate to URL\nagent-browser back            # Go back\nagent-browser forward         # Go forward\nagent-browser reload          # Reload page\nagent-browser close           # Close browser\n```\n\n### Snapshot (page analysis)\n\n```bash\nagent-browser snapshot            # Full accessibility tree\nagent-browser snapshot -i         # Interactive elements only (recommended)\nagent-browser snapshot -c         # Compact output\nagent-browser snapshot -d 3       # Limit depth to 3\nagent-browser snapshot -s \"#main\" # Scope to CSS selector\n```\n\n### Interactions (use @refs from snapshot)\n\n```bash\nagent-browser click @e1           # Click\nagent-browser dblclick @e1        # Double-click\nagent-browser focus @e1           # Focus element\nagent-browser fill @e2 \"text\"     # Clear and type\nagent-browser type @e2 \"text\"     # Type without clearing\nagent-browser press Enter         # Press key\nagent-browser press Control+a     # Key combination\nagent-browser keydown Shift       # Hold key down\nagent-browser keyup Shift         # Release key\nagent-browser hover @e1           # Hover\nagent-browser check @e1           # Check checkbox\nagent-browser uncheck @e1         # Uncheck checkbox\nagent-browser select @e1 \"value\"  # Select dropdown\nagent-browser scroll down 500     # Scroll page\nagent-browser scrollintoview @e1  # Scroll element into view\nagent-browser drag @e1 @e2        # Drag and drop\nagent-browser upload @e1 file.pdf # Upload files\n```\n\n### Get information\n\n```bash\nagent-browser get text @e1        # Get element text\nagent-browser get html @e1        # Get innerHTML\nagent-browser get value @e1       # Get input value\nagent-browser get attr @e1 href   # Get attribute\nagent-browser get title           # Get page title\nagent-browser get url             # Get current URL\nagent-browser get count \".item\"   # Count matching elements\nagent-browser get box @e1         # Get bounding box\n```\n\n### Check state\n\n```bash\nagent-browser is visible @e1      # Check if visible\nagent-browser is enabled @e1      # Check if enabled\nagent-browser is checked @e1      # Check if checked\n```\n\n### Screenshots & PDF\n\n```bash\nagent-browser screenshot          # Screenshot to stdout\nagent-browser screenshot path.png # Save to file\nagent-browser screenshot --full   # Full page\nagent-browser pdf output.pdf      # Save as PDF\n```\n\n### Video recording\n\n```bash\nagent-browser record start ./demo.webm    # Start recording (uses current URL + state)\nagent-browser click @e1                   # Perform actions\nagent-browser record stop                 # Stop and save video\nagent-browser record restart ./take2.webm # Stop current + start new recording\n```\n\nRecording creates a fresh context but preserves cookies/storage from your session.\n\n### Wait\n\n```bash\nagent-browser wait @e1                     # Wait for element\nagent-browser wait 2000                    # Wait milliseconds\nagent-browser wait --text \"Success\"        # Wait for text\nagent-browser wait --url \"**/dashboard\"    # Wait for URL pattern\nagent-browser wait --load networkidle      # Wait for network idle\nagent-browser wait --fn \"window.ready\"     # Wait for JS condition\n```\n\n### Mouse control\n\n```bash\nagent-browser mouse move 100 200      # Move mouse\nagent-browser mouse down left         # Press button\nagent-browser mouse up left           # Release button\nagent-browser mouse wheel 100         # Scroll wheel\n```\n\n### Semantic locators (alternative to refs)\n\n```bash\nagent-browser find role button click --name \"Submit\"\nagent-browser find text \"Sign In\" click\nagent-browser find label \"Email\" fill \"user@test.com\"\nagent-browser find first \".item\" click\nagent-browser find nth 2 \"a\" text\n```\n\n### Browser settings\n\n```bash\nagent-browser set viewport 1920 1080      # Set viewport size\nagent-browser set device \"iPhone 14\"      # Emulate device\nagent-browser set geo 37.7749 -122.4194   # Set geolocation\nagent-browser set offline on              # Toggle offline mode\nagent-browser set headers '{\"X-Key\":\"v\"}' # Extra HTTP headers\nagent-browser set credentials user pass   # HTTP basic auth\nagent-browser set media dark              # Emulate color scheme\n```\n\n### Cookies & Storage\n\n```bash\nagent-browser cookies                     # Get all cookies\nagent-browser cookies set name value      # Set cookie\nagent-browser cookies clear               # Clear cookies\nagent-browser storage local               # Get all localStorage\nagent-browser storage local key           # Get specific key\nagent-browser storage local set k v       # Set value\nagent-browser storage local clear         # Clear all\nagent-browser storage session             # Get all sessionStorage\nagent-browser storage session key         # Get specific key\nagent-browser storage session set k v     # Set value\nagent-browser storage session clear       # Clear all\n```\n\n### Network\n\n```bash\nagent-browser network route <url>              # Intercept requests\nagent-browser network route <url> --abort      # Block requests\nagent-browser network route <url> --body '{}'  # Mock response\nagent-browser network unroute [url]            # Remove routes\nagent-browser network requests                 # View tracked requests\nagent-browser network requests --filter api    # Filter requests\n```\n\n### Tabs & Windows\n\n```bash\nagent-browser tab                 # List tabs\nagent-browser tab new [url]       # New tab\nagent-browser tab 2               # Switch to tab\nagent-browser tab close           # Close tab\nagent-browser window new          # New window\n```\n\n### Frames\n\n```bash\nagent-browser frame \"#iframe\"     # Switch to iframe\nagent-browser frame main          # Back to main frame\n```\n\n### Dialogs\n\n```bash\nagent-browser dialog accept [text]  # Accept dialog\nagent-browser dialog dismiss        # Dismiss dialog\n```\n\n### JavaScript\n\n```bash\nagent-browser eval \"document.title\"   # Run JavaScript\n```\n\n## Global Options\n\n| Option                     | Description                                                 |\n| -------------------------- | ----------------------------------------------------------- |\n| `--session <name>`         | Isolated browser session (`AGENT_BROWSER_SESSION` env)      |\n| `--profile <path>`         | Persistent browser profile (`AGENT_BROWSER_PROFILE` env)    |\n| `--headers <json>`         | HTTP headers scoped to URL's origin                         |\n| `--executable-path <path>` | Custom browser binary (`AGENT_BROWSER_EXECUTABLE_PATH` env) |\n| `--args <args>`            | Browser launch args (`AGENT_BROWSER_ARGS` env)              |\n| `--user-agent <ua>`        | Custom User-Agent (`AGENT_BROWSER_USER_AGENT` env)          |\n| `--proxy <url>`            | Proxy server (`AGENT_BROWSER_PROXY` env)                    |\n| `--proxy-bypass <hosts>`   | Hosts to bypass proxy (`AGENT_BROWSER_PROXY_BYPASS` env)    |\n| `-p, --provider <name>`    | Cloud browser provider (`AGENT_BROWSER_PROVIDER` env)       |\n| `--json`                   | Machine-readable JSON output                                |\n| `--headed`                 | Show browser window (not headless)                          |\n| `--cdp <port\\|wss://url>`  | Connect via Chrome DevTools Protocol                        |\n| `--debug`                  | Debug output                                                |\n\n## Example: Form submission\n\n```bash\nagent-browser open https://example.com/form\nagent-browser snapshot -i\n# Output shows: textbox \"Email\" [ref=e1], textbox \"Password\" [ref=e2], button \"Submit\" [ref=e3]\n\nagent-browser fill @e1 \"user@example.com\"\nagent-browser fill @e2 \"password123\"\nagent-browser click @e3\nagent-browser wait --load networkidle\nagent-browser snapshot -i  # Check result\n```\n\n## Example: Authentication with saved state\n\n```bash\n# Login once\nagent-browser open https://app.example.com/login\nagent-browser snapshot -i\nagent-browser fill @e1 \"username\"\nagent-browser fill @e2 \"password\"\nagent-browser click @e3\nagent-browser wait --url \"**/dashboard\"\nagent-browser state save auth.json\n\n# Later sessions: load saved state\nagent-browser state load auth.json\nagent-browser open https://app.example.com/dashboard\n```\n\n### Header-based Auth (Skip login flows)\n\n```bash\n# Headers scoped to api.example.com only\nagent-browser open api.example.com --headers '{\"Authorization\": \"Bearer <token>\"}'\n# Navigate to another domain - headers NOT sent (safe)\nagent-browser open other-site.com\n# Global headers (all domains)\nagent-browser set headers '{\"X-Custom-Header\": \"value\"}'\n```\n\n## Sessions & Persistent Profiles\n\n### Sessions (parallel browsers)\n\n```bash\nagent-browser --session test1 open site-a.com\nagent-browser --session test2 open site-b.com\nagent-browser session list\n```\n\n### Persistent Profiles\n\nPersists cookies, localStorage, IndexedDB, service workers, cache, login sessions across browser restarts.\n\n```bash\nagent-browser --profile ~/.myapp-profile open myapp.com\n# Or via env var\nAGENT_BROWSER_PROFILE=~/.myapp-profile agent-browser open myapp.com\n```\n\n- Use different profile paths for different projects\n- Login once → restart browser → still logged in\n- Stores: cookies, localStorage, IndexedDB, service workers, browser cache\n\n## JSON output (for parsing)\n\nAdd `--json` for machine-readable output:\n\n```bash\nagent-browser snapshot -i --json\nagent-browser get text @e1 --json\n```\n\n## Debugging\n\n```bash\nagent-browser open example.com --headed              # Show browser window\nagent-browser console                                # View console messages\nagent-browser errors                                 # View page errors\nagent-browser record start ./debug.webm              # Record from current page\nagent-browser record stop                            # Save recording\nagent-browser connect 9222                           # Local CDP port\nagent-browser --cdp \"wss://browser-service.com/cdp?token=...\" snapshot  # Remote via WebSocket\nagent-browser console --clear                        # Clear console\nagent-browser errors --clear                         # Clear errors\nagent-browser highlight @e1                          # Highlight element\nagent-browser trace start                            # Start recording trace\nagent-browser trace stop trace.zip                   # Stop and save trace\n```\n\n---\n\n## Installation\n\n### Step 1: Install agent-browser CLI\n\n```bash\nbun add -g agent-browser\n```\n\n### Step 2: Install Playwright browsers\n\n**IMPORTANT**: `agent-browser install` may fail on some platforms (e.g., darwin-arm64) with \"No binary found\" error. In that case, install Playwright browsers directly:\n\n```bash\n# Create a temp project and install playwright\ncd /tmp && bun init -y && bun add playwright\n\n# Install Chromium browser\nbun playwright install chromium\n```\n\nThis downloads Chrome for Testing to `~/Library/Caches/ms-playwright/`.\n\n### Verify installation\n\n```bash\nagent-browser open https://example.com --headed\n```\n\nIf the browser opens successfully, installation is complete.\n\n### Troubleshooting\n\n| Error                                           | Solution                                                                      |\n| ----------------------------------------------- | ----------------------------------------------------------------------------- |\n| `No binary found for darwin-arm64`              | Run `bun playwright install chromium` in a project with playwright dependency |\n| `Executable doesn't exist at .../chromium-XXXX` | Re-run `bun playwright install chromium`                                      |\n| Browser doesn't open                            | Ensure `--headed` flag is used for visible browser                            |\n\n---\n\nRun `agent-browser --help` for all commands. Repo: https://github.com/vercel-labs/agent-browser",
    "allowedTools": [
      "Bash(agent-browser:*)"
    ]
  },
  {
    "name": "andrew-kane-gem-writer",
    "description": "This skill should be used when writing Ruby gems following Andrew Kane's proven patterns and philosophy. It applies when creating new Ruby gems, refactoring existing gems, designing gem APIs, or when clean, minimal, production-ready Ruby library code is needed. Triggers on requests like \"create a gem\", \"write a Ruby library\", \"design a gem API\", or mentions of Andrew Kane's style.",
    "template": "# Andrew Kane Gem Writer\n\nWrite Ruby gems following Andrew Kane's battle-tested patterns from 100+ gems with 374M+ downloads (Searchkick, PgHero, Chartkick, Strong Migrations, Lockbox, Ahoy, Blazer, Groupdate, Neighbor, Blind Index).\n\n## Core Philosophy\n\n**Simplicity over cleverness.** Zero or minimal dependencies. Explicit code over metaprogramming. Rails integration without Rails coupling. Every pattern serves production use cases.\n\n## Entry Point Structure\n\nEvery gem follows this exact pattern in `lib/gemname.rb`:\n\n```ruby\n# 1. Dependencies (stdlib preferred)\nrequire \"forwardable\"\n\n# 2. Internal modules\nrequire_relative \"gemname/model\"\nrequire_relative \"gemname/version\"\n\n# 3. Conditional Rails (CRITICAL - never require Rails directly)\nrequire_relative \"gemname/railtie\" if defined?(Rails)\n\n# 4. Module with config and errors\nmodule GemName\n  class Error < StandardError; end\n  class InvalidConfigError < Error; end\n\n  class << self\n    attr_accessor :timeout, :logger\n    attr_writer :client\n  end\n\n  self.timeout = 10  # Defaults set immediately\nend\n```\n\n## Class Macro DSL Pattern\n\nThe signature Kane pattern—single method call configures everything:\n\n```ruby\n# Usage\nclass Product < ApplicationRecord\n  searchkick word_start: [:name]\nend\n\n# Implementation\nmodule GemName\n  module Model\n    def gemname(**options)\n      unknown = options.keys - KNOWN_KEYWORDS\n      raise ArgumentError, \"unknown keywords: #{unknown.join(\", \")}\" if unknown.any?\n\n      mod = Module.new\n      mod.module_eval do\n        define_method :some_method do\n          # implementation\n        end unless method_defined?(:some_method)\n      end\n      include mod\n\n      class_eval do\n        cattr_reader :gemname_options, instance_reader: false\n        class_variable_set :@@gemname_options, options.dup\n      end\n    end\n  end\nend\n```\n\n## Rails Integration\n\n**Always use `ActiveSupport.on_load`—never require Rails gems directly:**\n\n```ruby\n# WRONG\nrequire \"active_record\"\nActiveRecord::Base.include(MyGem::Model)\n\n# CORRECT\nActiveSupport.on_load(:active_record) do\n  extend GemName::Model\nend\n\n# Use prepend for behavior modification\nActiveSupport.on_load(:active_record) do\n  ActiveRecord::Migration.prepend(GemName::Migration)\nend\n```\n\n## Configuration Pattern\n\nUse `class << self` with `attr_accessor`, not Configuration objects:\n\n```ruby\nmodule GemName\n  class << self\n    attr_accessor :timeout, :logger\n    attr_writer :master_key\n  end\n\n  def self.master_key\n    @master_key ||= ENV[\"GEMNAME_MASTER_KEY\"]\n  end\n\n  self.timeout = 10\n  self.logger = nil\nend\n```\n\n## Error Handling\n\nSimple hierarchy with informative messages:\n\n```ruby\nmodule GemName\n  class Error < StandardError; end\n  class ConfigError < Error; end\n  class ValidationError < Error; end\nend\n\n# Validate early with ArgumentError\ndef initialize(key:)\n  raise ArgumentError, \"Key must be 32 bytes\" unless key&.bytesize == 32\nend\n```\n\n## Testing (Minitest Only)\n\n```ruby\n# test/test_helper.rb\nrequire \"bundler/setup\"\nBundler.require(:default)\nrequire \"minitest/autorun\"\nrequire \"minitest/pride\"\n\n# test/model_test.rb\nclass ModelTest < Minitest::Test\n  def test_basic_functionality\n    assert_equal expected, actual\n  end\nend\n```\n\n## Gemspec Pattern\n\nZero runtime dependencies when possible:\n\n```ruby\nGem::Specification.new do |spec|\n  spec.name = \"gemname\"\n  spec.version = GemName::VERSION\n  spec.required_ruby_version = \">= 3.1\"\n  spec.files = Dir[\"*.{md,txt}\", \"{lib}/**/*\"]\n  spec.require_path = \"lib\"\n  # NO add_dependency lines - dev deps go in Gemfile\nend\n```\n\n## Anti-Patterns to Avoid\n\n- `method_missing` (use `define_method` instead)\n- Configuration objects (use class accessors)\n- `@@class_variables` (use `class << self`)\n- Requiring Rails gems directly\n- Many runtime dependencies\n- Committing Gemfile.lock in gems\n- RSpec (use Minitest)\n- Heavy DSLs (prefer explicit Ruby)\n\n## Reference Files\n\nFor deeper patterns, see:\n\n- **[references/module-organization.md](references/module-organization.md)** - Directory layouts, method decomposition\n- **[references/rails-integration.md](references/rails-integration.md)** - Railtie, Engine, on_load patterns\n- **[references/database-adapters.md](references/database-adapters.md)** - Multi-database support patterns\n- **[references/testing-patterns.md](references/testing-patterns.md)** - Multi-version testing, CI setup\n- **[references/resources.md](references/resources.md)** - Links to Kane's repos and articles"
  },
  {
    "name": "brainstorming",
    "description": "This skill should be used before implementing features, building components, or making changes. It guides exploring user intent, approaches, and design decisions before planning. Triggers on \"let's brainstorm\", \"help me think through\", \"what should we build\", \"explore approaches\", ambiguous feature requests, or when the user's request has multiple valid interpretations that need clarification.",
    "template": "# Brainstorming\n\nThis skill provides detailed process knowledge for effective brainstorming sessions that clarify **WHAT** to build before diving into **HOW** to build it.\n\n## When to Use This Skill\n\nBrainstorming is valuable when:\n\n- Requirements are unclear or ambiguous\n- Multiple approaches could solve the problem\n- Trade-offs need to be explored with the user\n- The user hasn't fully articulated what they want\n- The feature scope needs refinement\n\nBrainstorming can be skipped when:\n\n- Requirements are explicit and detailed\n- The user knows exactly what they want\n- The task is a straightforward bug fix or well-defined change\n\n## Core Process\n\n### Phase 0: Assess Requirement Clarity\n\nBefore diving into questions, assess whether brainstorming is needed.\n\n**Signals that requirements are clear:**\n\n- User provided specific acceptance criteria\n- User referenced existing patterns to follow\n- User described exact behavior expected\n- Scope is constrained and well-defined\n\n**Signals that brainstorming is needed:**\n\n- User used vague terms (\"make it better\", \"add something like\")\n- Multiple reasonable interpretations exist\n- Trade-offs haven't been discussed\n- User seems unsure about the approach\n\nIf requirements are clear, suggest: \"Your requirements seem clear. Consider proceeding directly to planning or implementation.\"\n\n### Phase 1: Understand the Idea\n\nAsk questions **one at a time** to understand the user's intent. Avoid overwhelming with multiple questions.\n\n**Question Techniques:**\n\n1. **Prefer multiple choice when natural options exist**\n   - Good: \"Should the notification be: (a) email only, (b) in-app only, or (c) both?\"\n   - Avoid: \"How should users be notified?\"\n\n2. **Start broad, then narrow**\n   - First: What is the core purpose?\n   - Then: Who are the users?\n   - Finally: What constraints exist?\n\n3. **Validate assumptions explicitly**\n   - \"I'm assuming users will be logged in. Is that correct?\"\n\n4. **Ask about success criteria early**\n   - \"How will you know this feature is working well?\"\n\n**Key Topics to Explore:**\n\n| Topic             | Example Questions                                     |\n| ----------------- | ----------------------------------------------------- |\n| Purpose           | What problem does this solve? What's the motivation?  |\n| Users             | Who uses this? What's their context?                  |\n| Constraints       | Any technical limitations? Timeline? Dependencies?    |\n| Success           | How will you measure success? What's the happy path?  |\n| Edge Cases        | What shouldn't happen? Any error states to consider?  |\n| Existing Patterns | Are there similar features in the codebase to follow? |\n\n**Exit Condition:** Continue until the idea is clear OR user says \"proceed\" or \"let's move on\"\n\n### Phase 2: Explore Approaches\n\nAfter understanding the idea, propose 2-3 concrete approaches.\n\n**Structure for Each Approach:**\n\n```markdown\n### Approach A: [Name]\n\n[2-3 sentence description]\n\n**Pros:**\n\n- [Benefit 1]\n- [Benefit 2]\n\n**Cons:**\n\n- [Drawback 1]\n- [Drawback 2]\n\n**Best when:** [Circumstances where this approach shines]\n```\n\n**Guidelines:**\n\n- Lead with a recommendation and explain why\n- Be honest about trade-offs\n- Consider YAGNI—simpler is usually better\n- Reference codebase patterns when relevant\n\n### Phase 3: Capture the Design\n\nSummarize key decisions in a structured format.\n\n**Design Doc Structure:**\n\n```markdown\n---\ndate: YYYY-MM-DD\ntopic: <kebab-case-topic>\n---\n\n# <Topic Title>\n\n## What We're Building\n\n[Concise description—1-2 paragraphs max]\n\n## Why This Approach\n\n[Brief explanation of approaches considered and why this one was chosen]\n\n## Key Decisions\n\n- [Decision 1]: [Rationale]\n- [Decision 2]: [Rationale]\n\n## Open Questions\n\n- [Any unresolved questions for the planning phase]\n\n## Next Steps\n\n→ `/workflows:plan` for implementation details\n```\n\n**Output Location:** `docs/brainstorms/YYYY-MM-DD-<topic>-brainstorm.md`\n\n### Phase 4: Handoff\n\nPresent clear options for what to do next:\n\n1. **Proceed to planning** → Run `/workflows:plan`\n2. **Refine further** → Continue exploring the design\n3. **Done for now** → User will return later\n\n## YAGNI Principles\n\nDuring brainstorming, actively resist complexity:\n\n- **Don't design for hypothetical future requirements**\n- **Choose the simplest approach that solves the stated problem**\n- **Prefer boring, proven patterns over clever solutions**\n- **Ask \"Do we really need this?\" when complexity emerges**\n- **Defer decisions that don't need to be made now**\n\n## Incremental Validation\n\nKeep sections short—200-300 words maximum. After each section of output, pause to validate understanding:\n\n- \"Does this match what you had in mind?\"\n- \"Any adjustments before we continue?\"\n- \"Is this the direction you want to go?\"\n\nThis prevents wasted effort on misaligned designs.\n\n## Anti-Patterns to Avoid\n\n| Anti-Pattern                          | Better Approach                             |\n| ------------------------------------- | ------------------------------------------- |\n| Asking 5 questions at once            | Ask one at a time                           |\n| Jumping to implementation details     | Stay focused on WHAT, not HOW               |\n| Proposing overly complex solutions    | Start simple, add complexity only if needed |\n| Ignoring existing codebase patterns   | Research what exists first                  |\n| Making assumptions without validating | State assumptions explicitly and confirm    |\n| Creating lengthy design documents     | Keep it concise—details go in the plan      |\n\n## Integration with Planning\n\nBrainstorming answers **WHAT** to build:\n\n- Requirements and acceptance criteria\n- Chosen approach and rationale\n- Key decisions and trade-offs\n\nPlanning answers **HOW** to build it:\n\n- Implementation steps and file changes\n- Technical details and code patterns\n- Testing strategy and verification\n\nWhen brainstorm output exists, `/workflows:plan` should detect it and use it as input, skipping its own idea refinement phase."
  },
  {
    "name": "coding-tutor",
    "description": "Personalized coding tutorials that build on your existing knowledge and use your actual codebase for examples. Creates a persistent learning trail that compounds over time using the power of AI, spaced repetition and quizes.",
    "template": "This skill creates personalized coding tutorials that evolve with the learner. Each tutorial builds on previous ones, uses real examples from the current codebase, and maintains a persistent record of concepts mastered.\n\nThe user asks to learn something - either a specific concept or an open \"teach me something new\" request.\n\nLegacy tutorial data at `~/coding-tutor-tutorials/` is not auto-imported. This skill exclusively reads/writes `~/ghostwire-tutorials/`.\n\n## Welcome New Learners\n\nIf `~/ghostwire-tutorials/` does not exist, this is a new learner. Before running setup, introduce yourself:\n\n> I'm your personal coding tutor. I create tutorials tailored to you - using real code from your projects, building on what you already know, and tracking your progress over time.\n>\n> All your tutorials live in one central library (`~/ghostwire-tutorials/`) that works across all your projects. Use `/ghostwire:teach-me` to learn something new, `/ghostwire:quiz-me` to test your retention with spaced repetition.\n\nThen proceed with setup and onboarding.\n\n## Setup: Ensure Tutorials Repo Exists\n\n**Before doing anything else**, run the setup script to ensure the central tutorials repository exists:\n\n```bash\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/coding-tutor/scripts/setup_tutorials.py\n```\n\nThis creates `~/ghostwire-tutorials/` if it doesn't exist. All tutorials and the learner profile are stored there, shared across all your projects.\n\n## First Step: Know Your Learner\n\n**Always start by reading `~/ghostwire-tutorials/learner_profile.md` if it exists.** This profile contains crucial context about who you're teaching - their background, goals, and personality. Use it to calibrate everything: what analogies will land, how fast to move, what examples resonate.\n\nIf no tutorials exist in `~/ghostwire-tutorials/` AND no learner profile exists at `~/ghostwire-tutorials/learner_profile.md`, this is a brand new learner. Before teaching anything, you need to understand who you're teaching.\n\n**Onboarding Interview:**\n\nAsk these three questions, one at a time. Wait for each answer before asking the next.\n\n1. **Prior exposure**: What's your background with programming? - Understand if they've built anything before, followed tutorials, or if this is completely new territory.\n\n2. **Ambitious goal**: This is your private AI tutor whose goal is to make you a top 1% programmer. Where do you want this to take you? - Understand what success looks like for them: a million-dollar product, a job at a company they admire, or something else entirely.\n\n3. **Who are you**: Tell me a bit about yourself - imagine we just met at a coworking space. - Get context that shapes how to teach them.\n\n4. **Optional**: Based on the above answers, you may ask upto one optional 4th question if it will make your understanding of the learner richer.\n\nAfter gathering responses, create `~/ghostwire-tutorials/learner_profile.md` and put the interview Q&A there (along with your commentary):\n\n```yaml\n---\ncreated: DD-MM-YYYY\nlast_updated: DD-MM-YYYY\n---\n\n**Q1. <insert question you asked>**\n**Answer**. <insert user's answer>\n**your internal commentary**\n\n**Q2. <insert question you asked>**\n**Answer**. <insert user's answer>\n**your internal commentary**\n\n**Q3. <insert question you asked>**\n**Answer**. <insert user's answer>\n**your internal commentary**\n\n**Q4. <optional>\n```\n\n## Teaching Philosophy\n\nOur general goal is to take the user from newbie to a senior engineer in record time. One at par with engineers at companies like 37 Signals or Vercel.\n\nBefore creating a tutorial, make a plan by following these steps:\n\n- **Load learner context**: Read `~/ghostwire-tutorials/learner_profile.md` to understand who you're teaching - their background, goals, and personality.\n- **Survey existing knowledge**: Run `python3 ${CLAUDE_PLUGIN_ROOT}/skills/coding-tutor/scripts/index_tutorials.py` to understand what concepts have been covered, at what depth, and how well they landed (understanding scores). Optionally, dive into particular tutorials in `~/ghostwire-tutorials/` to read them.\n- **Identify the gap**: What's the next concept that would be most valuable? Consider both what they've asked for AND what naturally follows from their current knowledge. Think of a curriculum that would get them from their current point to Senior Engineer - what should be the next 3 topics they need to learn to advance their programming knowledge in this direction?\n- **Find the anchor**: Locate real examples in the codebase that demonstrate this concept. Learning from abstract examples is forgettable; learning from YOUR code is sticky.\n- **(Optional) Use ask-user-question tool**: Ask clarifying questions to the learner to understand their intent, goals or expectations if it'll help you make a better plan.\n\nThen show this curriculum plan of **next 3 TUTORIALS** to the user and proceed to the tutorial creation step only if the user approves. If the user rejects, create a new plan using steps mentioned above.\n\n## Tutorial Creation\n\nEach tutorial is a markdown file in `~/ghostwire-tutorials/` with this structure:\n\n```yaml\n---\nconcepts: [primary_concept, related_concept_1, related_concept_2]\nsource_repo: my-app  # Auto-detected: which repo this tutorial's examples come from\ndescription: One-paragraph summary of what this tutorial covers\nunderstanding_score: null  # null until quizzed, then 1-10 based on quiz performance\nlast_quizzed: null  # null until first quiz, then DD-MM-YYYY\nprerequisites: [~/ghostwire-tutorials/tutorial_1_name.md, ~/ghostwire-tutorials/tutorial_2_name.md, (upto 3 other existing tutorials)]\ncreated: DD-MM-YYYY\nlast_updated: DD-MM-YYYY\n---\n\nFull contents of tutorial go here\n\n---\n\n## Q&A\n\nCross-questions during learning go here.\n\n## Quiz History\n\nQuiz sessions recorded here.\n```\n\nRun `scripts/create_tutorial.py` like this to create a new tutorial with template:\n\n```bash\npython3 ${CLAUDE_PLUGIN_ROOT}/skills/coding-tutor/scripts/create_tutorial.py \"Topic Name\" --concepts \"Concept1,Concept2\"\n```\n\nThis creates an empty template of the tutorial. Then you should edit the newly created file to write in the actual tutorial.\nQualities of a great tutorial should:\n\n- **Start with the \"why\"**: Not \"here's how callbacks work\" but \"here's the problem in your code that callbacks solve\"\n- **Use their code**: Every concept demonstrated with examples pulled from the actual codebase. Reference specific files and line numbers.\n- **Build mental models**: Diagrams, analogies, the underlying \"shape\" of the concept - not just syntax, ELI5\n- **Predict confusion**: Address the questions they're likely to ask before they ask them, don't skim over things, don't write in a notes style\n- **End with a challenge**: A small exercise they could try in this codebase to cement understanding\n\n### Tutorial Writing Style\n\nWrite personal tutorials like the best programming educators: Julia Evans, Dan Abramov. Not like study notes or documentation. There's a difference between a well-structured tutorial and one that truly teaches.\n\n- Show the struggle - \"Here's what you might try... here's why it doesn't work... here's the insight that unlocks it.\"\n- Fewer concepts, more depth - A tutorial that teaches 3 things deeply beats one that mentions 10 things.\n- Tell stories - a great tutorial is one coherent story, dives deep into a single concept, using storytelling techniques that engage readers\n\nWe should make the learner feel like Julia Evans or Dan Abramov is their private tutor.\n\nNote: If you're not sure about a fact or capability or new features/APIs, do web research, look at documentation to make sure you're teaching accurate up-to-date things. NEVER commit the sin of teaching something incorrect.\n\n## The Living Tutorial\n\nTutorials aren't static documents - they evolve:\n\n- **Q&A is mandatory**: When the learner asks ANY clarifying question about a tutorial, you MUST append it to the tutorial's `## Q&A` section. This is not optional - these exchanges are part of their personalized learning record and improve future teaching.\n- If the learner says they can't follow the tutorial or need you to take a different approach, update the tutorial like they ask\n- Update `last_updated` timestamp\n- If a question reveals a gap in prerequisites, note it for future tutorial planning\n\nNote: `understanding_score` is only updated through Quiz Mode, not during teaching.\n\n## What Makes Great Teaching\n\n**DO**: Meet them where they are. Use their vocabulary. Reference their past struggles. Make connections to concepts they already own. Be encouraging but honest about complexity.\n\n**DON'T**: Assume knowledge not demonstrated in previous tutorials. Use generic blog-post examples when codebase examples exist. Overwhelm with every edge case upfront. Be condescending about gaps.\n\n**CALIBRATE**: A learner with 3 tutorials is different from one with 30. Early tutorials need more scaffolding and encouragement. Later tutorials can move faster and reference the shared history you've built.\n\nRemember: The goal isn't to teach programming in the abstract. It's to teach THIS person, using THEIR code, building on THEIR specific journey. Every tutorial should feel like it was written specifically for them - because it was.\n\n## Quiz Mode\n\nTutorials teach. Quizzes verify. The score should reflect what the learner actually retained, not what was presented to them.\n\n**Triggers:**\n\n- Explicit: \"Quiz me on React hooks\" → quiz that specific concept\n- Open: \"Quiz me on something\" → run `python3 ${CLAUDE_PLUGIN_ROOT}/skills/coding-tutor/scripts/quiz_priority.py` to get a prioritized list based on spaced repetition, then choose what to quiz\n\n**Spaced Repetition:**\n\nWhen the user requests an open quiz, the priority script uses spaced repetition intervals to surface:\n\n- Never-quizzed tutorials (need baseline assessment)\n- Low-scored concepts that are overdue for review\n- High-scored concepts whose review interval has elapsed\n\nThe script uses Fibonacci-ish intervals: score 1 = review in 2 days, score 5 = 13 days, score 8 = 55 days, score 10 = 144 days. This means weak concepts get drilled frequently while mastered ones fade into long-term review.\n\nThe script gives you an ordered list with `understanding_score` and `last_quizzed` for each tutorial. Use this to make an informed choice about what to quiz, and explain to the learner why you picked that concept (\"You learned callbacks 5 days ago but scored 4/10 - let's see if it's sticking better now\").\n\n**Philosophy:**\n\nA quiz isn't an exam - it's a conversation that reveals understanding. Ask questions that expose mental models, not just syntax recall. The goal is to find the edges of their knowledge: where does solid understanding fade into uncertainty?\n\n**Ask only 1 question at a time.** Wait for the learner's answer before asking the next question.\n\nMix question types based on what the concept demands:\n\n- Conceptual (\"when would you use X over Y?\")\n- Code reading (\"what does this code in your app do?\")\n- Code writing (\"write a scope that does X\")\n- Debugging (\"what's wrong here?\")\n\nUse their codebase for examples whenever possible. \"What does line 47 of `app/models/user.rb` do?\" is more valuable than abstract snippets.\n\n**Scoring:**\n\nAfter the quiz, update `understanding_score` honestly:\n\n- **1-3**: Can't recall the concept, needs re-teaching\n- **4-5**: Vague memory, partial answers\n- **6-7**: Solid understanding, minor gaps\n- **8-9**: Strong grasp, handles edge cases\n- **10**: Could teach this to someone else\n\nAlso update `last_quizzed: DD-MM-YYYY` in the frontmatter.\n\n**Recording:**\n\nAppend to the tutorial's `## Quiz History` section:\n\n```\n### Quiz - DD-MM-YYYY\n**Q:** [Question asked]\n**A:** [Brief summary of their response and what it revealed about understanding]\nScore updated: 5 → 7\n```\n\nThis history helps future quizzes avoid repetition and track progression over time."
  },
  {
    "name": "creating-agent-skills",
    "description": "Expert guidance for creating, writing, and refining Claude Code Skills. Use when working with SKILL.md files, authoring new skills, improving existing skills, or understanding skill structure and best practices.",
    "template": "# Creating Agent Skills\n\nThis skill teaches how to create effective Claude Code Skills following Anthropic's official specification.\n\n## Core Principles\n\n### 1. Skills Are Prompts\n\nAll prompting best practices apply. Be clear, be direct. Assume Claude is smart - only add context Claude doesn't have.\n\n### 2. Standard Markdown Format\n\nUse YAML frontmatter + markdown body. **No XML tags** - use standard markdown headings.\n\n```markdown\n---\nname: my-skill-name\ndescription: What it does and when to use it\n---\n\n# My Skill Name\n\n## Quick Start\n\nImmediate actionable guidance...\n\n## Instructions\n\nStep-by-step procedures...\n\n## Examples\n\nConcrete usage examples...\n```\n\n### 3. Progressive Disclosure\n\nKeep SKILL.md under 500 lines. Split detailed content into reference files. Load only what's needed.\n\n```\nmy-skill/\n├── SKILL.md              # Entry point (required)\n├── reference.md          # Detailed docs (loaded when needed)\n├── examples.md           # Usage examples\n└── scripts/              # Utility scripts (executed, not loaded)\n```\n\n### 4. Effective Descriptions\n\nThe description field enables skill discovery. Include both what the skill does AND when to use it. Write in third person.\n\n**Good:**\n\n```yaml\ndescription: Extracts text and tables from PDF files, fills forms, merges documents. Use when working with PDF files or when the user mentions PDFs, forms, or document extraction.\n```\n\n**Bad:**\n\n```yaml\ndescription: Helps with documents\n```\n\n## Skill Structure\n\n### Required Frontmatter\n\n| Field           | Required | Max Length | Description                              |\n| --------------- | -------- | ---------- | ---------------------------------------- |\n| `name`          | Yes      | 64 chars   | Lowercase letters, numbers, hyphens only |\n| `description`   | Yes      | 1024 chars | What it does AND when to use it          |\n| `allowed-tools` | No       | -          | Tools Claude can use without asking      |\n| `model`         | No       | -          | Specific model to use                    |\n\n### Naming Conventions\n\nUse **gerund form** (verb + -ing) for skill names:\n\n- `processing-pdfs`\n- `analyzing-spreadsheets`\n- `generating-commit-messages`\n- `reviewing-code`\n\nAvoid: `helper`, `utils`, `tools`, `anthropic-*`, `claude-*`\n\n### Body Structure\n\nUse standard markdown headings:\n\n```markdown\n# Skill Name\n\n## Quick Start\n\nFastest path to value...\n\n## Instructions\n\nCore guidance Claude follows...\n\n## Examples\n\nInput/output pairs showing expected behavior...\n\n## Advanced Features\n\nAdditional capabilities (link to reference files)...\n\n## Guidelines\n\nRules and constraints...\n```\n\n## What Would You Like To Do?\n\n1. **Create new skill** - Build from scratch\n2. **Audit existing skill** - Check against best practices\n3. **Add component** - Add workflow/reference/example\n4. **Get guidance** - Understand skill design\n\n## Creating a New Skill\n\n### Step 1: Choose Type\n\n**Simple skill (single file):**\n\n- Under 500 lines\n- Self-contained guidance\n- No complex workflows\n\n**Progressive disclosure skill (multiple files):**\n\n- SKILL.md as overview\n- Reference files for detailed docs\n- Scripts for utilities\n\n### Step 2: Create SKILL.md\n\n````markdown\n---\nname: your-skill-name\ndescription: [What it does]. Use when [trigger conditions].\n---\n\n# Your Skill Name\n\n## Quick Start\n\n[Immediate actionable example]\n\n```[language]\n[Code example]\n```\n````\n\n## Instructions\n\n[Core guidance]\n\n## Examples\n\n**Example 1:**\nInput: [description]\nOutput:\n\n```\n[result]\n```\n\n## Guidelines\n\n- [Constraint 1]\n- [Constraint 2]\n\n````\n\n### Step 3: Add Reference Files (If Needed)\n\nLink from SKILL.md to detailed content:\n\n```markdown\nFor API reference, see [REFERENCE.md](REFERENCE.md).\nFor form filling guide, see [FORMS.md](FORMS.md).\n````\n\nKeep references **one level deep** from SKILL.md.\n\n### Step 4: Add Scripts (If Needed)\n\nScripts execute without loading into context:\n\n````markdown\n## Utility Scripts\n\nExtract fields:\n\n```bash\npython scripts/analyze.py input.pdf > fields.json\n```\n````\n\n````\n\n### Step 5: Test With Real Usage\n\n1. Test with actual tasks, not test scenarios\n2. Observe where Claude struggles\n3. Refine based on real behavior\n4. Test with Haiku, Sonnet, and Opus\n\n## Auditing Existing Skills\n\nCheck against this rubric:\n\n- [ ] Valid YAML frontmatter (name + description)\n- [ ] Description includes trigger keywords\n- [ ] Uses standard markdown headings (not XML tags)\n- [ ] SKILL.md under 500 lines\n- [ ] References one level deep\n- [ ] Examples are concrete, not abstract\n- [ ] Consistent terminology\n- [ ] No time-sensitive information\n- [ ] Scripts handle errors explicitly\n\n## Common Patterns\n\n### Template Pattern\n\nProvide output templates for consistent results:\n\n```markdown\n## Report Template\n\n```markdown\n# [Analysis Title]\n\n## Executive Summary\n[One paragraph overview]\n\n## Key Findings\n- Finding 1\n- Finding 2\n\n## Recommendations\n1. [Action item]\n2. [Action item]\n````\n\n````\n\n### Workflow Pattern\n\nFor complex multi-step tasks:\n\n```markdown\n## Migration Workflow\n\nCopy this checklist:\n\n````\n\n- [ ] Step 1: Backup database\n- [ ] Step 2: Run migration script\n- [ ] Step 3: Validate output\n- [ ] Step 4: Update configuration\n\n```\n\n**Step 1: Backup database**\nRun: `./scripts/backup.sh`\n...\n```\n\n### Conditional Pattern\n\nGuide through decision points:\n\n```markdown\n## Choose Your Approach\n\n**Creating new content?** Follow \"Creation workflow\" below.\n**Editing existing?** Follow \"Editing workflow\" below.\n```\n\n## Anti-Patterns to Avoid\n\n- **XML tags in body** - Use markdown headings instead\n- **Vague descriptions** - Be specific with trigger keywords\n- **Deep nesting** - Keep references one level from SKILL.md\n- **Too many options** - Provide a default with escape hatch\n- **Windows paths** - Always use forward slashes\n- **Punting to Claude** - Scripts should handle errors\n- **Time-sensitive info** - Use \"old patterns\" section instead\n\n## Reference Files\n\nFor detailed guidance, see:\n\n- [official-spec.md](references/official-spec.md) - Anthropic's official skill specification\n- [best-practices.md](references/best-practices.md) - Skill authoring best practices\n\n## Success Criteria\n\nA well-structured skill:\n\n- Has valid YAML frontmatter with descriptive name and description\n- Uses standard markdown headings (not XML tags)\n- Keeps SKILL.md under 500 lines\n- Links to reference files for detailed content\n- Includes concrete examples with input/output pairs\n- Has been tested with real usage\n\nSources:\n\n- [Agent Skills - Claude Code Docs](https://code.claude.com/docs/en/skills)\n- [Skill authoring best practices](https://platform.claude.com/docs/en/agents-and-tools/agent-skills/best-practices)\n- [GitHub - anthropics/skills](https://github.com/anthropics/skills)"
  },
  {
    "name": "dev-browser",
    "description": "Browser automation with persistent page state. Use when users ask to navigate websites, fill forms, take screenshots, extract web data, test web apps, or automate browser workflows. Trigger phrases include \"go to [url]\", \"click on\", \"fill out the form\", \"take a screenshot\", \"scrape\", \"automate\", \"test the website\", \"log into\", or any browser interaction request.",
    "template": "# Dev Browser Skill\n\nBrowser automation that maintains page state across script executions. Write small, focused scripts to accomplish tasks incrementally. Once you've proven out part of a workflow and there is repeated work to be done, you can write a script to do the repeated work in a single execution.\n\n## Choosing Your Approach\n\n- **Local/source-available sites**: Read the source code first to write selectors directly\n- **Unknown page layouts**: Use `getAISnapshot()` to discover elements and `selectSnapshotRef()` to interact with them\n- **Visual feedback**: Take screenshots to see what the user sees\n\n## Setup\n\n> **Installation**: See [references/installation.md](references/installation.md) for detailed setup instructions including Windows support.\n\nTwo modes available. Ask the user if unclear which to use.\n\n### Standalone Mode (Default)\n\nLaunches a new Chromium browser for fresh automation sessions.\n\n```bash\n./skills/dev-browser/server.sh &\n```\n\nAdd `--headless` flag if user requests it. **Wait for the `Ready` message before running scripts.**\n\n### Extension Mode\n\nConnects to user's existing Chrome browser. Use this when:\n\n- The user is already logged into sites and wants you to do things behind an authed experience that isn't local dev.\n- The user asks you to use the extension\n\n**Important**: The core flow is still the same. You create named pages inside of their browser.\n\n**Start the relay server:**\n\n```bash\ncd skills/dev-browser && npm i && npm run start-extension &\n```\n\nWait for `Waiting for extension to connect...` followed by `Extension connected` in the console. To know that a client has connected and the browser is ready to be controlled.\n**Workflow:**\n\n1. Scripts call `client.page(\"name\")` just like the normal mode to create new pages / connect to existing ones.\n2. Automation runs on the user's actual browser session\n\nIf the extension hasn't connected yet, tell the user to launch and activate it. Download link: https://github.com/SawyerHood/dev-browser/releases\n\n## Writing Scripts\n\n> **Run all scripts from `skills/dev-browser/` directory.** The `@/` import alias requires this directory's config.\n\nExecute scripts inline using heredocs:\n\n```bash\ncd skills/dev-browser && npx tsx <<'EOF'\nimport { connect, waitForPageLoad } from \"@/client.js\";\n\nconst client = await connect();\n// Create page with custom viewport size (optional)\nconst page = await client.page(\"example\", { viewport: { width: 1920, height: 1080 } });\n\nawait page.goto(\"https://example.com\");\nawait waitForPageLoad(page);\n\nconsole.log({ title: await page.title(), url: page.url() });\nawait client.disconnect();\nEOF\n```\n\n**Write to `tmp/` files only when** the script needs reuse, is complex, or user explicitly requests it.\n\n### Key Principles\n\n1. **Small scripts**: Each script does ONE thing (navigate, click, fill, check)\n2. **Evaluate state**: Log/return state at the end to decide next steps\n3. **Descriptive page names**: Use `\"checkout\"`, `\"login\"`, not `\"main\"`\n4. **Disconnect to exit**: `await client.disconnect()` - pages persist on server\n5. **Plain JS in evaluate**: `page.evaluate()` runs in browser - no TypeScript syntax\n\n## Workflow Loop\n\nFollow this pattern for complex tasks:\n\n1. **Write a script** to perform one action\n2. **Run it** and observe the output\n3. **Evaluate** - did it work? What's the current state?\n4. **Decide** - is the task complete or do we need another script?\n5. **Repeat** until task is done\n\n### No TypeScript in Browser Context\n\nCode passed to `page.evaluate()` runs in the browser, which doesn't understand TypeScript:\n\n```typescript\n// ✅ Correct: plain JavaScript\nconst text = await page.evaluate(() => {\n  return document.body.innerText;\n});\n\n// ❌ Wrong: TypeScript syntax will fail at runtime\nconst text = await page.evaluate(() => {\n  const el: HTMLElement = document.body; // Type annotation breaks in browser!\n  return el.innerText;\n});\n```\n\n## Scraping Data\n\nFor scraping large datasets, intercept and replay network requests rather than scrolling the DOM. See [references/scraping.md](references/scraping.md) for the complete guide covering request capture, schema discovery, and paginated API replay.\n\n## Client API\n\n```typescript\nconst client = await connect();\n\n// Get or create named page (viewport only applies to new pages)\nconst page = await client.page(\"name\");\nconst pageWithSize = await client.page(\"name\", { viewport: { width: 1920, height: 1080 } });\n\nconst pages = await client.list(); // List all page names\nawait client.close(\"name\"); // Close a page\nawait client.disconnect(); // Disconnect (pages persist)\n\n// ARIA Snapshot methods\nconst snapshot = await client.getAISnapshot(\"name\"); // Get accessibility tree\nconst element = await client.selectSnapshotRef(\"name\", \"e5\"); // Get element by ref\n```\n\nThe `page` object is a standard Playwright Page.\n\n## Waiting\n\n```typescript\nimport { waitForPageLoad } from \"@/client.js\";\n\nawait waitForPageLoad(page); // After navigation\nawait page.waitForSelector(\".results\"); // For specific elements\nawait page.waitForURL(\"**/success\"); // For specific URL\n```\n\n## Inspecting Page State\n\n### Screenshots\n\n```typescript\nawait page.screenshot({ path: \"tmp/screenshot.png\" });\nawait page.screenshot({ path: \"tmp/full.png\", fullPage: true });\n```\n\n### ARIA Snapshot (Element Discovery)\n\nUse `getAISnapshot()` to discover page elements. Returns YAML-formatted accessibility tree:\n\n```yaml\n- banner:\n  - link \"Hacker News\" [ref=e1]\n  - navigation:\n    - link \"new\" [ref=e2]\n- main:\n  - list:\n    - listitem:\n      - link \"Article Title\" [ref=e8]\n      - link \"328 comments\" [ref=e9]\n- contentinfo:\n  - textbox [ref=e10]\n    - /placeholder: \"Search\"\n```\n\n**Interpreting refs:**\n\n- `[ref=eN]` - Element reference for interaction (visible, clickable elements only)\n- `[checked]`, `[disabled]`, `[expanded]` - Element states\n- `[level=N]` - Heading level\n- `/url:`, `/placeholder:` - Element properties\n\n**Interacting with refs:**\n\n```typescript\nconst snapshot = await client.getAISnapshot(\"hackernews\");\nconsole.log(snapshot); // Find the ref you need\n\nconst element = await client.selectSnapshotRef(\"hackernews\", \"e2\");\nawait element.click();\n```\n\n## Error Recovery\n\nPage state persists after failures. Debug with:\n\n```bash\ncd skills/dev-browser && npx tsx <<'EOF'\nimport { connect } from \"@/client.js\";\n\nconst client = await connect();\nconst page = await client.page(\"hackernews\");\n\nawait page.screenshot({ path: \"tmp/debug.png\" });\nconsole.log({\n  url: page.url(),\n  title: await page.title(),\n  bodyText: await page.textContent(\"body\").then((t) => t?.slice(0, 200)),\n});\n\nawait client.disconnect();\nEOF\n```"
  },
  {
    "name": "dhh-rails-style",
    "description": "This skill should be used when writing Ruby and Rails code in DHH's distinctive 37signals style. It applies when writing Ruby code, Rails applications, creating models, controllers, or any Ruby file. Triggers on Ruby/Rails code generation, refactoring requests, code review, or when the user mentions DHH, 37signals, Basecamp, HEY, or Campfire style. Embodies REST purity, fat models, thin controllers, Current attributes, Hotwire patterns, and the \"clarity over cleverness\" philosophy.",
    "template": "<objective>\nApply 37signals/DHH Rails conventions to Ruby and Rails code. This skill provides comprehensive domain expertise extracted from analyzing production 37signals codebases (Fizzy/Campfire) and DHH's code review patterns.\n</objective>\n\n<essential_principles>\n\n## Core Philosophy\n\n\"The best code is the code you don't write. The second best is the code that's obviously correct.\"\n\n**Vanilla Rails is plenty:**\n\n- Rich domain models over service objects\n- CRUD controllers over custom actions\n- Concerns for horizontal code sharing\n- Records as state instead of boolean columns\n- Database-backed everything (no Redis)\n- Build solutions before reaching for gems\n\n**What they deliberately avoid:**\n\n- devise (custom ~150-line auth instead)\n- pundit/cancancan (simple role checks in models)\n- sidekiq (Solid Queue uses database)\n- redis (database for everything)\n- view_component (partials work fine)\n- GraphQL (REST with Turbo sufficient)\n- factory_bot (fixtures are simpler)\n- rspec (Minitest ships with Rails)\n- Tailwind (native CSS with layers)\n\n**Development Philosophy:**\n\n- Ship, Validate, Refine - prototype-quality code to production to learn\n- Fix root causes, not symptoms\n- Write-time operations over read-time computations\n- Database constraints over ActiveRecord validations\n  </essential_principles>\n\n<intake>\nWhat are you working on?\n\n1. **Controllers** - REST mapping, concerns, Turbo responses, API patterns\n2. **Models** - Concerns, state records, callbacks, scopes, POROs\n3. **Views & Frontend** - Turbo, Stimulus, CSS, partials\n4. **Architecture** - Routing, multi-tenancy, authentication, jobs, caching\n5. **Testing** - Minitest, fixtures, integration tests\n6. **Gems & Dependencies** - What to use vs avoid\n7. **Code Review** - Review code against DHH style\n8. **General Guidance** - Philosophy and conventions\n\n**Specify a number or describe your task.**\n</intake>\n\n<routing>\n\n| Response                                   | Reference to Read                               |\n| ------------------------------------------ | ----------------------------------------------- |\n| 1, controller                              | [controllers.md](./references/controllers.md)   |\n| 2, model                                   | [models.md](./references/models.md)             |\n| 3, view, frontend, turbo, stimulus, css    | [frontend.md](./references/frontend.md)         |\n| 4, architecture, routing, auth, job, cache | [architecture.md](./references/architecture.md) |\n| 5, test, testing, minitest, fixture        | [testing.md](./references/testing.md)           |\n| 6, gem, dependency, library                | [gems.md](./references/gems.md)                 |\n| 7, review                                  | Read all references, then review code           |\n| 8, general task                            | Read relevant references based on context       |\n\n**After reading relevant references, apply patterns to the user's code.**\n</routing>\n\n<quick_reference>\n\n## Naming Conventions\n\n**Verbs:** `card.close`, `card.gild`, `board.publish` (not `set_style` methods)\n\n**Predicates:** `card.closed?`, `card.golden?` (derived from presence of related record)\n\n**Concerns:** Adjectives describing capability (`Closeable`, `Publishable`, `Watchable`)\n\n**Controllers:** Nouns matching resources (`Cards::ClosuresController`)\n\n**Scopes:**\n\n- `chronologically`, `reverse_chronologically`, `alphabetically`, `latest`\n- `preloaded` (standard eager loading name)\n- `indexed_by`, `sorted_by` (parameterized)\n- `active`, `unassigned` (business terms, not SQL-ish)\n\n## REST Mapping\n\nInstead of custom actions, create new resources:\n\n```\nPOST /cards/:id/close    → POST /cards/:id/closure\nDELETE /cards/:id/close  → DELETE /cards/:id/closure\nPOST /cards/:id/archive  → POST /cards/:id/archival\n```\n\n## Ruby Syntax Preferences\n\n```ruby\n# Symbol arrays with spaces inside brackets\nbefore_action :set_message, only: %i[ show edit update destroy ]\n\n# Private method indentation\n  private\n    def set_message\n      @message = Message.find(params[:id])\n    end\n\n# Expression-less case for conditionals\ncase\nwhen params[:before].present?\n  messages.page_before(params[:before])\nelse\n  messages.last_page\nend\n\n# Bang methods for fail-fast\n@message = Message.create!(params)\n\n# Ternaries for simple conditionals\n@room.direct? ? @room.users : @message.mentionees\n```\n\n## Key Patterns\n\n**State as Records:**\n\n```ruby\nCard.joins(:closure)         # closed cards\nCard.where.missing(:closure) # open cards\n```\n\n**Current Attributes:**\n\n```ruby\nbelongs_to :creator, default: -> { Current.user }\n```\n\n**Authorization on Models:**\n\n```ruby\nclass User < ApplicationRecord\n  def can_administer?(message)\n    message.creator == self || admin?\n  end\nend\n```\n\n</quick_reference>\n\n<reference_index>\n\n## Domain Knowledge\n\nAll detailed patterns in `references/`:\n\n| File                                            | Topics                                                                         |\n| ----------------------------------------------- | ------------------------------------------------------------------------------ |\n| [controllers.md](./references/controllers.md)   | REST mapping, concerns, Turbo responses, API patterns, HTTP caching            |\n| [models.md](./references/models.md)             | Concerns, state records, callbacks, scopes, POROs, authorization, broadcasting |\n| [frontend.md](./references/frontend.md)         | Turbo Streams, Stimulus controllers, CSS layers, OKLCH colors, partials        |\n| [architecture.md](./references/architecture.md) | Routing, authentication, jobs, Current attributes, caching, database patterns  |\n| [testing.md](./references/testing.md)           | Minitest, fixtures, unit/integration/system tests, testing patterns            |\n| [gems.md](./references/gems.md)                 | What they use vs avoid, decision framework, Gemfile examples                   |\n\n</reference_index>\n\n<success_criteria>\nCode follows DHH style when:\n\n- Controllers map to CRUD verbs on resources\n- Models use concerns for horizontal behavior\n- State is tracked via records, not booleans\n- No unnecessary service objects or abstractions\n- Database-backed solutions preferred over external services\n- Tests use Minitest with fixtures\n- Turbo/Stimulus for interactivity (no heavy JS frameworks)\n- Native CSS with modern features (layers, OKLCH, nesting)\n- Authorization logic lives on User model\n- Jobs are shallow wrappers calling model methods\n  </success_criteria>\n\n<credits>\nBased on [The Unofficial 37signals/DHH Rails Style Guide](https://github.com/marckohlbrugge/unofficial-37signals-coding-style-guide) by [Marc Köhlbrugge](https://x.com/marckohlbrugge), generated through deep analysis of 265 pull requests from the Fizzy codebase.\n\n**Important Disclaimers:**\n\n- LLM-generated guide - may contain inaccuracies\n- Code examples from Fizzy are licensed under the O'Saasy License\n- Not affiliated with or endorsed by 37signals\n  </credits>"
  },
  {
    "name": "dspy-ruby",
    "description": "This skill should be used when working with DSPy.rb, a Ruby framework for building type-safe, composable LLM applications. Use this when implementing predictable AI features, creating LLM signatures and modules, configuring language model providers (OpenAI, Anthropic, Gemini, Ollama), building agent systems with tools, optimizing prompts, or testing LLM-powered functionality in Ruby applications.",
    "template": "# DSPy.rb Expert\n\n## Overview\n\nDSPy.rb is a Ruby framework that enables developers to **program LLMs, not prompt them**. Instead of manually crafting prompts, define application requirements through type-safe, composable modules that can be tested, optimized, and version-controlled like regular code.\n\nThis skill provides comprehensive guidance on:\n\n- Creating type-safe signatures for LLM operations\n- Building composable modules and workflows\n- Configuring multiple LLM providers\n- Implementing agents with tools\n- Testing and optimizing LLM applications\n- Production deployment patterns\n\n## Core Capabilities\n\n### 1. Type-Safe Signatures\n\nCreate input/output contracts for LLM operations with runtime type checking.\n\n**When to use**: Defining any LLM task, from simple classification to complex analysis.\n\n**Quick reference**:\n\n```ruby\nclass EmailClassificationSignature < DSPy::Signature\n  description \"Classify customer support emails\"\n\n  input do\n    const :email_subject, String\n    const :email_body, String\n  end\n\n  output do\n    const :category, T.enum([\"Technical\", \"Billing\", \"General\"])\n    const :priority, T.enum([\"Low\", \"Medium\", \"High\"])\n  end\nend\n```\n\n**Templates**: See `assets/signature-template.rb` for comprehensive examples including:\n\n- Basic signatures with multiple field types\n- Vision signatures for multimodal tasks\n- Sentiment analysis signatures\n- Code generation signatures\n\n**Best practices**:\n\n- Always provide clear, specific descriptions\n- Use enums for constrained outputs\n- Include field descriptions with `desc:` parameter\n- Prefer specific types over generic String when possible\n\n**Full documentation**: See `references/core-concepts.md` sections on Signatures and Type Safety.\n\n### 2. Composable Modules\n\nBuild reusable, chainable modules that encapsulate LLM operations.\n\n**When to use**: Implementing any LLM-powered feature, especially complex multi-step workflows.\n\n**Quick reference**:\n\n```ruby\nclass EmailProcessor < DSPy::Module\n  def initialize\n    super\n    @classifier = DSPy::Predict.new(EmailClassificationSignature)\n  end\n\n  def forward(email_subject:, email_body:)\n    @classifier.forward(\n      email_subject: email_subject,\n      email_body: email_body\n    )\n  end\nend\n```\n\n**Templates**: See `assets/module-template.rb` for comprehensive examples including:\n\n- Basic modules with single predictors\n- Multi-step pipelines that chain modules\n- Modules with conditional logic\n- Error handling and retry patterns\n- Stateful modules with history\n- Caching implementations\n\n**Module composition**: Chain modules together to create complex workflows:\n\n```ruby\nclass Pipeline < DSPy::Module\n  def initialize\n    super\n    @step1 = Classifier.new\n    @step2 = Analyzer.new\n    @step3 = Responder.new\n  end\n\n  def forward(input)\n    result1 = @step1.forward(input)\n    result2 = @step2.forward(result1)\n    @step3.forward(result2)\n  end\nend\n```\n\n**Full documentation**: See `references/core-concepts.md` sections on Modules and Module Composition.\n\n### 3. Multiple Predictor Types\n\nChoose the right predictor for your task:\n\n**Predict**: Basic LLM inference with type-safe inputs/outputs\n\n```ruby\npredictor = DSPy::Predict.new(TaskSignature)\nresult = predictor.forward(input: \"data\")\n```\n\n**ChainOfThought**: Adds automatic reasoning for improved accuracy\n\n```ruby\npredictor = DSPy::ChainOfThought.new(TaskSignature)\nresult = predictor.forward(input: \"data\")\n# Returns: { reasoning: \"...\", output: \"...\" }\n```\n\n**ReAct**: Tool-using agents with iterative reasoning\n\n```ruby\npredictor = DSPy::ReAct.new(\n  TaskSignature,\n  tools: [SearchTool.new, CalculatorTool.new],\n  max_iterations: 5\n)\n```\n\n**CodeAct**: Dynamic code generation (requires `dspy-code_act` gem)\n\n```ruby\npredictor = DSPy::CodeAct.new(TaskSignature)\nresult = predictor.forward(task: \"Calculate factorial of 5\")\n```\n\n**When to use each**:\n\n- **Predict**: Simple tasks, classification, extraction\n- **ChainOfThought**: Complex reasoning, analysis, multi-step thinking\n- **ReAct**: Tasks requiring external tools (search, calculation, API calls)\n- **CodeAct**: Tasks best solved with generated code\n\n**Full documentation**: See `references/core-concepts.md` section on Predictors.\n\n### 4. LLM Provider Configuration\n\nSupport for OpenAI, Anthropic Claude, Google Gemini, Ollama, and OpenRouter.\n\n**Quick configuration examples**:\n\n```ruby\n# OpenAI\nDSPy.configure do |c|\n  c.lm = DSPy::LM.new('openai/gpt-4o-mini',\n    api_key: ENV['OPENAI_API_KEY'])\nend\n\n# Anthropic Claude\nDSPy.configure do |c|\n  c.lm = DSPy::LM.new('anthropic/claude-3-5-sonnet-20241022',\n    api_key: ENV['ANTHROPIC_API_KEY'])\nend\n\n# Google Gemini\nDSPy.configure do |c|\n  c.lm = DSPy::LM.new('gemini/gemini-1.5-pro',\n    api_key: ENV['GOOGLE_API_KEY'])\nend\n\n# Local Ollama (free, private)\nDSPy.configure do |c|\n  c.lm = DSPy::LM.new('ollama/llama3.1')\nend\n```\n\n**Templates**: See `assets/config-template.rb` for comprehensive examples including:\n\n- Environment-based configuration\n- Multi-model setups for different tasks\n- Configuration with observability (OpenTelemetry, Langfuse)\n- Retry logic and fallback strategies\n- Budget tracking\n- Rails initializer patterns\n\n**Provider compatibility matrix**:\n\n| Feature           | OpenAI | Anthropic | Gemini | Ollama     |\n| ----------------- | ------ | --------- | ------ | ---------- |\n| Structured Output | ✅     | ✅        | ✅     | ✅         |\n| Vision (Images)   | ✅     | ✅        | ✅     | ⚠️ Limited |\n| Image URLs        | ✅     | ❌        | ❌     | ❌         |\n| Tool Calling      | ✅     | ✅        | ✅     | Varies     |\n\n**Cost optimization strategy**:\n\n- Development: Ollama (free) or gpt-4o-mini (cheap)\n- Testing: gpt-4o-mini with temperature=0.0\n- Production simple tasks: gpt-4o-mini, claude-3-haiku, gemini-1.5-flash\n- Production complex tasks: gpt-4o, claude-3-5-sonnet, gemini-1.5-pro\n\n**Full documentation**: See `references/providers.md` for all configuration options, provider-specific features, and troubleshooting.\n\n### 5. Multimodal & Vision Support\n\nProcess images alongside text using the unified `DSPy::Image` interface.\n\n**Quick reference**:\n\n```ruby\nclass VisionSignature < DSPy::Signature\n  description \"Analyze image and answer questions\"\n\n  input do\n    const :image, DSPy::Image\n    const :question, String\n  end\n\n  output do\n    const :answer, String\n  end\nend\n\npredictor = DSPy::Predict.new(VisionSignature)\nresult = predictor.forward(\n  image: DSPy::Image.from_file(\"path/to/image.jpg\"),\n  question: \"What objects are visible?\"\n)\n```\n\n**Image loading methods**:\n\n```ruby\n# From file\nDSPy::Image.from_file(\"path/to/image.jpg\")\n\n# From URL (OpenAI only)\nDSPy::Image.from_url(\"https://example.com/image.jpg\")\n\n# From base64\nDSPy::Image.from_base64(base64_data, mime_type: \"image/jpeg\")\n```\n\n**Provider support**:\n\n- OpenAI: Full support including URLs\n- Anthropic, Gemini: Base64 or file loading only\n- Ollama: Limited multimodal depending on model\n\n**Full documentation**: See `references/core-concepts.md` section on Multimodal Support.\n\n### 6. Testing LLM Applications\n\nWrite standard RSpec tests for LLM logic.\n\n**Quick reference**:\n\n```ruby\nRSpec.describe EmailClassifier do\n  before do\n    DSPy.configure do |c|\n      c.lm = DSPy::LM.new('openai/gpt-4o-mini',\n        api_key: ENV['OPENAI_API_KEY'])\n    end\n  end\n\n  it 'classifies technical emails correctly' do\n    classifier = EmailClassifier.new\n    result = classifier.forward(\n      email_subject: \"Can't log in\",\n      email_body: \"Unable to access account\"\n    )\n\n    expect(result[:category]).to eq('Technical')\n    expect(result[:priority]).to be_in(['High', 'Medium', 'Low'])\n  end\nend\n```\n\n**Testing patterns**:\n\n- Mock LLM responses for unit tests\n- Use VCR for deterministic API testing\n- Test type safety and validation\n- Test edge cases (empty inputs, special characters, long texts)\n- Integration test complete workflows\n\n**Full documentation**: See `references/optimization.md` section on Testing.\n\n### 7. Optimization & Improvement\n\nAutomatically improve prompts and modules using optimization techniques.\n\n**MIPROv2 optimization**:\n\n```ruby\nrequire 'dspy/mipro'\n\n# Define evaluation metric\ndef accuracy_metric(example, prediction)\n  example[:expected_output][:category] == prediction[:category] ? 1.0 : 0.0\nend\n\n# Prepare training data\ntraining_examples = [\n  {\n    input: { email_subject: \"...\", email_body: \"...\" },\n    expected_output: { category: 'Technical' }\n  },\n  # More examples...\n]\n\n# Run optimization\noptimizer = DSPy::MIPROv2.new(\n  metric: method(:accuracy_metric),\n  num_candidates: 10\n)\n\noptimized_module = optimizer.compile(\n  EmailClassifier.new,\n  trainset: training_examples\n)\n```\n\n**A/B testing different approaches**:\n\n```ruby\n# Test ChainOfThought vs ReAct\napproach_a_score = evaluate_approach(ChainOfThoughtModule, test_set)\napproach_b_score = evaluate_approach(ReActModule, test_set)\n```\n\n**Full documentation**: See `references/optimization.md` section on Optimization.\n\n### 8. Observability & Monitoring\n\nTrack performance, token usage, and behavior in production.\n\n**OpenTelemetry integration**:\n\n```ruby\nrequire 'opentelemetry/sdk'\n\nOpenTelemetry::SDK.configure do |c|\n  c.service_name = 'my-dspy-app'\n  c.use_all\nend\n\n# DSPy automatically creates traces\n```\n\n**Langfuse tracing**:\n\n```ruby\nDSPy.configure do |c|\n  c.lm = DSPy::LM.new('openai/gpt-4o-mini',\n    api_key: ENV['OPENAI_API_KEY'])\n\n  c.langfuse = {\n    public_key: ENV['LANGFUSE_PUBLIC_KEY'],\n    secret_key: ENV['LANGFUSE_SECRET_KEY']\n  }\nend\n```\n\n**Custom monitoring**:\n\n- Token tracking\n- Performance monitoring\n- Error rate tracking\n- Custom logging\n\n**Full documentation**: See `references/optimization.md` section on Observability.\n\n## Quick Start Workflow\n\n### For New Projects\n\n1. **Install DSPy.rb and provider gems**:\n\n```bash\ngem install dspy dspy-openai  # or dspy-anthropic, dspy-gemini\n```\n\n2. **Configure LLM provider** (see `assets/config-template.rb`):\n\n```ruby\nrequire 'dspy'\n\nDSPy.configure do |c|\n  c.lm = DSPy::LM.new('openai/gpt-4o-mini',\n    api_key: ENV['OPENAI_API_KEY'])\nend\n```\n\n3. **Create a signature** (see `assets/signature-template.rb`):\n\n```ruby\nclass MySignature < DSPy::Signature\n  description \"Clear description of task\"\n\n  input do\n    const :input_field, String, desc: \"Description\"\n  end\n\n  output do\n    const :output_field, String, desc: \"Description\"\n  end\nend\n```\n\n4. **Create a module** (see `assets/module-template.rb`):\n\n```ruby\nclass MyModule < DSPy::Module\n  def initialize\n    super\n    @predictor = DSPy::Predict.new(MySignature)\n  end\n\n  def forward(input_field:)\n    @predictor.forward(input_field: input_field)\n  end\nend\n```\n\n5. **Use the module**:\n\n```ruby\nmodule_instance = MyModule.new\nresult = module_instance.forward(input_field: \"test\")\nputs result[:output_field]\n```\n\n6. **Add tests** (see `references/optimization.md`):\n\n```ruby\nRSpec.describe MyModule do\n  it 'produces expected output' do\n    result = MyModule.new.forward(input_field: \"test\")\n    expect(result[:output_field]).to be_a(String)\n  end\nend\n```\n\n### For Rails Applications\n\n1. **Add to Gemfile**:\n\n```ruby\ngem 'dspy'\ngem 'dspy-openai'  # or other provider\n```\n\n2. **Create initializer** at `config/initializers/dspy.rb` (see `assets/config-template.rb` for full example):\n\n```ruby\nrequire 'dspy'\n\nDSPy.configure do |c|\n  c.lm = DSPy::LM.new('openai/gpt-4o-mini',\n    api_key: ENV['OPENAI_API_KEY'])\nend\n```\n\n3. **Create modules in** `app/llm/` directory:\n\n```ruby\n# app/llm/email_classifier.rb\nclass EmailClassifier < DSPy::Module\n  # Implementation here\nend\n```\n\n4. **Use in controllers/services**:\n\n```ruby\nclass EmailsController < ApplicationController\n  def classify\n    classifier = EmailClassifier.new\n    result = classifier.forward(\n      email_subject: params[:subject],\n      email_body: params[:body]\n    )\n    render json: result\n  end\nend\n```\n\n## Common Patterns\n\n### Pattern: Multi-Step Analysis Pipeline\n\n```ruby\nclass AnalysisPipeline < DSPy::Module\n  def initialize\n    super\n    @extract = DSPy::Predict.new(ExtractSignature)\n    @analyze = DSPy::ChainOfThought.new(AnalyzeSignature)\n    @summarize = DSPy::Predict.new(SummarizeSignature)\n  end\n\n  def forward(text:)\n    extracted = @extract.forward(text: text)\n    analyzed = @analyze.forward(data: extracted[:data])\n    @summarize.forward(analysis: analyzed[:result])\n  end\nend\n```\n\n### Pattern: Agent with Tools\n\n```ruby\nclass ResearchAgent < DSPy::Module\n  def initialize\n    super\n    @agent = DSPy::ReAct.new(\n      ResearchSignature,\n      tools: [\n        WebSearchTool.new,\n        DatabaseQueryTool.new,\n        SummarizerTool.new\n      ],\n      max_iterations: 10\n    )\n  end\n\n  def forward(question:)\n    @agent.forward(question: question)\n  end\nend\n\nclass WebSearchTool < DSPy::Tool\n  def call(query:)\n    results = perform_search(query)\n    { results: results }\n  end\nend\n```\n\n### Pattern: Conditional Routing\n\n```ruby\nclass SmartRouter < DSPy::Module\n  def initialize\n    super\n    @classifier = DSPy::Predict.new(ClassifySignature)\n    @simple_handler = SimpleModule.new\n    @complex_handler = ComplexModule.new\n  end\n\n  def forward(input:)\n    classification = @classifier.forward(text: input)\n\n    if classification[:complexity] == 'Simple'\n      @simple_handler.forward(input: input)\n    else\n      @complex_handler.forward(input: input)\n    end\n  end\nend\n```\n\n### Pattern: Retry with Fallback\n\n```ruby\nclass RobustModule < DSPy::Module\n  MAX_RETRIES = 3\n\n  def forward(input, retry_count: 0)\n    begin\n      @predictor.forward(input)\n    rescue DSPy::ValidationError => e\n      if retry_count < MAX_RETRIES\n        sleep(2 ** retry_count)\n        forward(input, retry_count: retry_count + 1)\n      else\n        # Fallback to default or raise\n        raise\n      end\n    end\n  end\nend\n```\n\n## Resources\n\nThis skill includes comprehensive reference materials and templates:\n\n### References (load as needed for detailed information)\n\n- [core-concepts.md](./references/core-concepts.md): Complete guide to signatures, modules, predictors, multimodal support, and best practices\n- [providers.md](./references/providers.md): All LLM provider configurations, compatibility matrix, cost optimization, and troubleshooting\n- [optimization.md](./references/optimization.md): Testing patterns, optimization techniques, observability setup, and monitoring\n\n### Assets (templates for quick starts)\n\n- [signature-template.rb](./assets/signature-template.rb): Examples of signatures including basic, vision, sentiment analysis, and code generation\n- [module-template.rb](./assets/module-template.rb): Module patterns including pipelines, agents, error handling, caching, and state management\n- [config-template.rb](./assets/config-template.rb): Configuration examples for all providers, environments, observability, and production patterns\n\n## When to Use This Skill\n\nTrigger this skill when:\n\n- Implementing LLM-powered features in Ruby applications\n- Creating type-safe interfaces for AI operations\n- Building agent systems with tool usage\n- Setting up or troubleshooting LLM providers\n- Optimizing prompts and improving accuracy\n- Testing LLM functionality\n- Adding observability to AI applications\n- Converting from manual prompt engineering to programmatic approach\n- Debugging DSPy.rb code or configuration issues"
  },
  {
    "name": "every-style-editor",
    "description": "This skill should be used when reviewing or editing copy to ensure adherence to Every's style guide. It provides a systematic line-by-line review process for grammar, punctuation, mechanics, and style guide compliance.",
    "template": "# Every Style Editor\n\nThis skill provides a systematic approach to reviewing copy against Every's comprehensive style guide. It transforms Claude into a meticulous line editor and proofreader specializing in grammar, mechanics, and style guide compliance.\n\n## When to Use This Skill\n\nUse this skill when:\n\n- Reviewing articles, blog posts, newsletters, or any written content\n- Ensuring copy follows Every's specific style conventions\n- Providing feedback on grammar, punctuation, and mechanics\n- Flagging deviations from the Every style guide\n- Preparing clean copy for human editorial review\n\n## Skill Overview\n\nThis skill enables performing a comprehensive review of written content in four phases:\n\n1. **Initial Assessment** - Understanding context and document type\n2. **Detailed Line Edit** - Checking every sentence for compliance\n3. **Mechanical Review** - Verifying formatting and consistency\n4. **Recommendations** - Providing actionable improvement suggestions\n\n## How to Use This Skill\n\n### Step 1: Initial Assessment\n\nBegin by reading the entire piece to understand:\n\n- Document type (article, knowledge base entry, social post, etc.)\n- Target audience\n- Overall tone and voice\n- Content context\n\n### Step 2: Detailed Line Edit\n\nReview each paragraph systematically, checking for:\n\n- Sentence structure and grammar correctness\n- Punctuation usage (commas, semicolons, em dashes, etc.)\n- Capitalization rules (especially job titles, headlines)\n- Word choice and usage (overused words, passive voice)\n- Adherence to Every style guide rules\n\nReference the complete [EVERY_WRITE_STYLE.md](./references/EVERY_WRITE_STYLE.md) for specific rules when in doubt.\n\n### Step 3: Mechanical Review\n\nVerify:\n\n- Spacing and formatting consistency\n- Style choices applied uniformly throughout\n- Special elements (lists, quotes, citations)\n- Proper use of italics and formatting\n- Number formatting (numerals vs. spelled out)\n- Link formatting and descriptions\n\n### Step 4: Output Results\n\nPresent findings using this structure:\n\n```\nDOCUMENT REVIEW SUMMARY\n=====================\nDocument Type: [type]\nWord Count: [approximate]\nOverall Assessment: [brief overview]\n\nERRORS FOUND: [total number]\n\nDETAILED CORRECTIONS\n===================\n\n[For each error found:]\n\n**Location**: [Paragraph #, Sentence #]\n**Issue Type**: [Grammar/Punctuation/Mechanics/Style Guide]\n**Original**: \"[exact text with error]\"\n**Correction**: \"[corrected text]\"\n**Rule Reference**: [Specific style guide rule violated]\n**Explanation**: [Brief explanation of why this is an error]\n\n---\n\nRECURRING ISSUES\n===============\n[List patterns of errors that appear multiple times]\n\nSTYLE GUIDE COMPLIANCE CHECKLIST\n==============================\n✓ [Rule followed correctly]\n✗ [Rule violated - with count of violations]\n\nFINAL RECOMMENDATIONS\n===================\n[2-3 actionable suggestions for improving the draft]\n```\n\n## Style Guide Reference\n\nThe complete Every style guide is included in [EVERY_WRITE_STYLE.md](./references/EVERY_WRITE_STYLE.md). Key areas to focus on:\n\n- **Quick Rules**: Title case for headlines, sentence case elsewhere\n- **Tone**: Active voice, avoid overused words (actually, very, just), be specific\n- **Numbers**: Spell out one through nine; use numerals for 10+\n- **Punctuation**: Oxford commas, em dashes without spaces, proper quotation mark usage\n- **Capitalization**: Lowercase job titles, company as singular (it), teams as plural (they)\n- **Emphasis**: Italics only (no bold for emphasis)\n- **Links**: 2-4 words, don't say \"click here\"\n\n## Key Principles\n\n- **Be specific**: Always quote the exact text with the error\n- **Reference rules**: Cite the specific style guide rule for each correction\n- **Maintain voice**: Preserve the author's voice while correcting errors\n- **Prioritize clarity**: Focus on changes that improve readability\n- **Be constructive**: Frame feedback to help writers improve\n- **Flag ambiguous cases**: When style guide doesn't address an issue, explain options and recommend the clearest choice\n\n## Common Areas to Focus On\n\nBased on Every's style guide, pay special attention to:\n\n- Punctuation (comma usage, semicolons, apostrophes, quotation marks)\n- Capitalization (proper nouns, titles, sentence starts)\n- Numbers (when to spell out vs. use numerals)\n- Passive voice (replace with active whenever possible)\n- Overused words (actually, very, just)\n- Lists (parallel structure, punctuation, capitalization)\n- Hyphenation (compound adjectives, except adverbs)\n- Word usage (fewer vs. less, they vs. them)\n- Company references (singular \"it\", teams as plural \"they\")\n- Job title capitalization"
  },
  {
    "name": "file-todos",
    "description": "This skill should be used when managing the file-based todo tracking system in the todos/ directory. It provides workflows for creating todos, managing status and dependencies, conducting triage, and integrating with slash commands and code review processes.",
    "template": "# File-Based Todo Tracking Skill\n\n## Overview\n\nThe `todos/` directory contains a file-based tracking system for managing code review feedback, technical debt, feature requests, and work items. Each todo is a markdown file with YAML frontmatter and structured sections.\n\nThis skill should be used when:\n\n- Creating new todos from findings or feedback\n- Managing todo lifecycle (pending → ready → complete)\n- Triaging pending items for approval\n- Checking or managing dependencies\n- Converting PR comments or code findings into tracked work\n- Updating work logs during todo execution\n\n## File Naming Convention\n\nTodo files follow this naming pattern:\n\n```\n{issue_id}-{status}-{priority}-{description}.md\n```\n\n**Components:**\n\n- **issue_id**: Sequential number (001, 002, 003...) - never reused\n- **status**: `pending` (needs triage), `ready` (approved), `complete` (done)\n- **priority**: `p1` (critical), `p2` (important), `p3` (nice-to-have)\n- **description**: kebab-case, brief description\n\n**Examples:**\n\n```\n001-pending-p1-mailer-test.md\n002-ready-p1-fix-n-plus-1.md\n005-complete-p2-refactor-csv.md\n```\n\n## File Structure\n\nEach todo is a markdown file with YAML frontmatter and structured sections. Use the template at [todo-template.md](./assets/todo-template.md) as a starting point when creating new todos.\n\n**Required sections:**\n\n- **Problem Statement** - What is broken, missing, or needs improvement?\n- **Findings** - Investigation results, root cause, key discoveries\n- **Proposed Solutions** - Multiple options with pros/cons, effort, risk\n- **Recommended Action** - Clear plan (filled during triage)\n- **Acceptance Criteria** - Testable checklist items\n- **Work Log** - Chronological record with date, actions, learnings\n\n**Optional sections:**\n\n- **Technical Details** - Affected files, related components, DB changes\n- **Resources** - Links to errors, tests, PRs, documentation\n- **Notes** - Additional context or decisions\n\n**YAML frontmatter fields:**\n\n```yaml\n---\nstatus: ready # pending | ready | complete\npriority: p1 # p1 | p2 | p3\nissue_id: \"002\"\ntags: [rails, performance, database]\ndependencies: [\"001\"] # Issue IDs this is blocked by\n---\n```\n\n## Common Workflows\n\n### Creating a New Todo\n\n**To create a new todo from findings or feedback:**\n\n1. Determine next issue ID: `ls todos/ | grep -o '^[0-9]\\+' | sort -n | tail -1`\n2. Copy template: `cp assets/todo-template.md todos/{NEXT_ID}-pending-{priority}-{description}.md`\n3. Edit and fill required sections:\n   - Problem Statement\n   - Findings (if from investigation)\n   - Proposed Solutions (multiple options)\n   - Acceptance Criteria\n   - Add initial Work Log entry\n4. Determine status: `pending` (needs triage) or `ready` (pre-approved)\n5. Add relevant tags for filtering\n\n**When to create a todo:**\n\n- Requires more than 15-20 minutes of work\n- Needs research, planning, or multiple approaches considered\n- Has dependencies on other work\n- Requires manager approval or prioritization\n- Part of larger feature or refactor\n- Technical debt needing documentation\n\n**When to act immediately instead:**\n\n- Issue is trivial (< 15 minutes)\n- Complete context available now\n- No planning needed\n- User explicitly requests immediate action\n- Simple bug fix with obvious solution\n\n### Triaging Pending Items\n\n**To triage pending todos:**\n\n1. List pending items: `ls todos/*-pending-*.md`\n2. For each todo:\n   - Read Problem Statement and Findings\n   - Review Proposed Solutions\n   - Make decision: approve, defer, or modify priority\n3. Update approved todos:\n   - Rename file: `mv {file}-pending-{pri}-{desc}.md {file}-ready-{pri}-{desc}.md`\n   - Update frontmatter: `status: pending` → `status: ready`\n   - Fill \"Recommended Action\" section with clear plan\n   - Adjust priority if different from initial assessment\n4. Deferred todos stay in `pending` status\n\n**Use slash command:** `/triage` for interactive approval workflow\n\n### Managing Dependencies\n\n**To track dependencies:**\n\n```yaml\ndependencies: [\"002\", \"005\"]  # This todo blocked by issues 002 and 005\ndependencies: []               # No blockers - can work immediately\n```\n\n**To check what blocks a todo:**\n\n```bash\ngrep \"^dependencies:\" todos/003-*.md\n```\n\n**To find what a todo blocks:**\n\n```bash\ngrep -l 'dependencies:.*\"002\"' todos/*.md\n```\n\n**To verify blockers are complete before starting:**\n\n```bash\nfor dep in 001 002 003; do\n  [ -f \"todos/${dep}-complete-*.md\" ] || echo \"Issue $dep not complete\"\ndone\n```\n\n### Updating Work Logs\n\n**When working on a todo, always add a work log entry:**\n\n```markdown\n### YYYY-MM-DD - Session Title\n\n**By:** Claude Code / Developer Name\n\n**Actions:**\n\n- Specific changes made (include file:line references)\n- Commands executed\n- Tests run\n- Results of investigation\n\n**Learnings:**\n\n- What worked / what didn't\n- Patterns discovered\n- Key insights for future work\n```\n\nWork logs serve as:\n\n- Historical record of investigation\n- Documentation of approaches attempted\n- Knowledge sharing for team\n- Context for future similar work\n\n### Completing a Todo\n\n**To mark a todo as complete:**\n\n1. Verify all acceptance criteria checked off\n2. Update Work Log with final session and results\n3. Rename file: `mv {file}-ready-{pri}-{desc}.md {file}-complete-{pri}-{desc}.md`\n4. Update frontmatter: `status: ready` → `status: complete`\n5. Check for unblocked work: `grep -l 'dependencies:.*\"002\"' todos/*-ready-*.md`\n6. Commit with issue reference: `feat: resolve issue 002`\n\n## Integration with Development Workflows\n\n| Trigger     | Flow                                               | Tool                 |\n| ----------- | -------------------------------------------------- | -------------------- |\n| Code review | `/workflows:review` → Findings → `/triage` → Todos | Review agent + skill |\n| PR comments | `/resolve_pr_parallel` → Individual fixes → Todos  | gh CLI + skill       |\n| Code TODOs  | `/resolve_todo_parallel` → Fixes + Complex todos   | Agent + skill        |\n| Planning    | Brainstorm → Create todo → Work → Complete         | Skill                |\n| Feedback    | Discussion → Create todo → Triage → Work           | Skill + slash        |\n\n## Quick Reference Commands\n\n**Finding work:**\n\n```bash\n# List highest priority unblocked work\ngrep -l 'dependencies: \\[\\]' todos/*-ready-p1-*.md\n\n# List all pending items needing triage\nls todos/*-pending-*.md\n\n# Find next issue ID\nls todos/ | grep -o '^[0-9]\\+' | sort -n | tail -1 | awk '{printf \"%03d\", $1+1}'\n\n# Count by status\nfor status in pending ready complete; do\n  echo \"$status: $(ls -1 todos/*-$status-*.md 2>/dev/null | wc -l)\"\ndone\n```\n\n**Dependency management:**\n\n```bash\n# What blocks this todo?\ngrep \"^dependencies:\" todos/003-*.md\n\n# What does this todo block?\ngrep -l 'dependencies:.*\"002\"' todos/*.md\n```\n\n**Searching:**\n\n```bash\n# Search by tag\ngrep -l \"tags:.*rails\" todos/*.md\n\n# Search by priority\nls todos/*-p1-*.md\n\n# Full-text search\ngrep -r \"payment\" todos/\n```\n\n## Key Distinctions\n\n**File-todos system (this skill):**\n\n- Markdown files in `todos/` directory\n- Development/project tracking\n- Standalone markdown files with YAML frontmatter\n- Used by humans and agents\n\n**Rails Todo model:**\n\n- Database model in `app/models/todo.rb`\n- User-facing feature in the application\n- Active Record CRUD operations\n- Different from this file-based system\n\n**TodoWrite tool:**\n\n- In-memory task tracking during agent sessions\n- Temporary tracking for single conversation\n- Not persisted to disk\n- Different from both systems above"
  },
  {
    "name": "frontend-design",
    "description": "This skill should be used when creating distinctive, production-grade frontend interfaces with high design quality. It applies when the user asks to build web components, pages, or applications. Generates creative, polished code that avoids generic AI aesthetics.",
    "template": "This skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What's the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend's aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don't hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision."
  },
  {
    "name": "frontend-ui-ux",
    "description": "Designer-turned-developer who crafts stunning UI/UX even without design mockups",
    "template": "# Role: Designer-Turned-Developer\n\nYou are a designer who learned to code. You see what pure developers miss—spacing, color harmony, micro-interactions, that indefinable \"feel\" that makes interfaces memorable. Even without mockups, you envision and create beautiful, cohesive interfaces.\n\n**Mission**: Create visually stunning, emotionally engaging interfaces users fall in love with. Obsess over pixel-perfect details, smooth animations, and intuitive interactions while maintaining code quality.\n\n---\n\n# Work Principles\n\n1. **Complete what's asked** — Execute the exact task. No scope creep. Work until it works. Never mark work complete without proper verification.\n2. **Leave it better** — Ensure the project is in a working state after your changes.\n3. **Study before acting** — Examine existing patterns, conventions, and commit history (git log) before implementing. Understand why code is structured the way it is.\n4. **Blend seamlessly** — Match existing code patterns. Your code should look like the team wrote it.\n5. **Be transparent** — Announce each step. Explain reasoning. Report both successes and failures.\n\n---\n\n# Design Process\n\nBefore coding, commit to a **BOLD aesthetic direction**:\n\n1. **Purpose**: What problem does this solve? Who uses it?\n2. **Tone**: Pick an extreme—brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian\n3. **Constraints**: Technical requirements (framework, performance, accessibility)\n4. **Differentiation**: What's the ONE thing someone will remember?\n\n**Key**: Choose a clear direction and execute with precision. Intentionality > intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, Angular, etc.) that is:\n\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n---\n\n# Aesthetic Guidelines\n\n## Typography\n\nChoose distinctive fonts. **Avoid**: Arial, Inter, Roboto, system fonts, Space Grotesk. Pair a characterful display font with a refined body font.\n\n## Color\n\nCommit to a cohesive palette. Use CSS variables. Dominant colors with sharp accents outperform timid, evenly-distributed palettes. **Avoid**: purple gradients on white (AI slop).\n\n## Motion\n\nFocus on high-impact moments. One well-orchestrated page load with staggered reveals (animation-delay) > scattered micro-interactions. Use scroll-triggering and hover states that surprise. Prioritize CSS-only. Use Motion library for React when available.\n\n## Spatial Composition\n\nUnexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n\n## Visual Details\n\nCreate atmosphere and depth—gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, grain overlays. Never default to solid colors.\n\n---\n\n# Anti-Patterns (NEVER)\n\n- Generic fonts (Inter, Roboto, Arial, system fonts, Space Grotesk)\n- Cliched color schemes (purple gradients on white)\n- Predictable layouts and component patterns\n- Cookie-cutter design lacking context-specific character\n- Converging on common choices across generations\n\n---\n\n# Execution\n\nMatch implementation complexity to aesthetic vision:\n\n- **Maximalist** → Elaborate code with extensive animations and effects\n- **Minimalist** → Restraint, precision, careful spacing and typography\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. You are capable of extraordinary creative work—don't hold back."
  },
  {
    "name": "gemini-imagegen",
    "description": "This skill should be used when generating and editing images using the Gemini API (Nano Banana Pro). It applies when creating images from text prompts, editing existing images, applying style transfers, generating logos with text, creating stickers, product mockups, or any image generation/manipulation task. Supports text-to-image, image editing, multi-turn refinement, and composition from multiple reference images.",
    "template": "# Gemini Image Generation (Nano Banana Pro)\n\nGenerate and edit images using Google's Gemini API. The environment variable `GEMINI_API_KEY` must be set.\n\n## Default Model\n\n| Model                        | Resolution | Best For                       |\n| ---------------------------- | ---------- | ------------------------------ |\n| `gemini-3-pro-image-preview` | 1K-4K      | All image generation (default) |\n\n**Note:** Always use this Pro model. Only use a different model if explicitly requested.\n\n## Quick Reference\n\n### Default Settings\n\n- **Model:** `gemini-3-pro-image-preview`\n- **Resolution:** 1K (default, options: 1K, 2K, 4K)\n- **Aspect Ratio:** 1:1 (default)\n\n### Available Aspect Ratios\n\n`1:1`, `2:3`, `3:2`, `3:4`, `4:3`, `4:5`, `5:4`, `9:16`, `16:9`, `21:9`\n\n### Available Resolutions\n\n`1K` (default), `2K`, `4K`\n\n## Core API Pattern\n\n```python\nimport os\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n\n# Basic generation (1K, 1:1 - defaults)\nresponse = client.models.generate_content(\n    model=\"gemini-3-pro-image-preview\",\n    contents=[\"Your prompt here\"],\n    config=types.GenerateContentConfig(\n        response_modalities=['TEXT', 'IMAGE'],\n    ),\n)\n\nfor part in response.parts:\n    if part.text:\n        print(part.text)\n    elif part.inline_data:\n        image = part.as_image()\n        image.save(\"output.png\")\n```\n\n## Custom Resolution & Aspect Ratio\n\n```python\nfrom google.genai import types\n\nresponse = client.models.generate_content(\n    model=\"gemini-3-pro-image-preview\",\n    contents=[prompt],\n    config=types.GenerateContentConfig(\n        response_modalities=['TEXT', 'IMAGE'],\n        image_config=types.ImageConfig(\n            aspect_ratio=\"16:9\",  # Wide format\n            image_size=\"2K\"       # Higher resolution\n        ),\n    )\n)\n```\n\n### Resolution Examples\n\n```python\n# 1K (default) - Fast, good for previews\nimage_config=types.ImageConfig(image_size=\"1K\")\n\n# 2K - Balanced quality/speed\nimage_config=types.ImageConfig(image_size=\"2K\")\n\n# 4K - Maximum quality, slower\nimage_config=types.ImageConfig(image_size=\"4K\")\n```\n\n### Aspect Ratio Examples\n\n```python\n# Square (default)\nimage_config=types.ImageConfig(aspect_ratio=\"1:1\")\n\n# Landscape wide\nimage_config=types.ImageConfig(aspect_ratio=\"16:9\")\n\n# Ultra-wide panoramic\nimage_config=types.ImageConfig(aspect_ratio=\"21:9\")\n\n# Portrait\nimage_config=types.ImageConfig(aspect_ratio=\"9:16\")\n\n# Photo standard\nimage_config=types.ImageConfig(aspect_ratio=\"4:3\")\n```\n\n## Editing Images\n\nPass existing images with text prompts:\n\n```python\nfrom PIL import Image\n\nimg = Image.open(\"input.png\")\nresponse = client.models.generate_content(\n    model=\"gemini-3-pro-image-preview\",\n    contents=[\"Add a sunset to this scene\", img],\n    config=types.GenerateContentConfig(\n        response_modalities=['TEXT', 'IMAGE'],\n    ),\n)\n```\n\n## Multi-Turn Refinement\n\nUse chat for iterative editing:\n\n```python\nfrom google.genai import types\n\nchat = client.chats.create(\n    model=\"gemini-3-pro-image-preview\",\n    config=types.GenerateContentConfig(response_modalities=['TEXT', 'IMAGE'])\n)\n\nresponse = chat.send_message(\"Create a logo for 'Acme Corp'\")\n# Save first image...\n\nresponse = chat.send_message(\"Make the text bolder and add a blue gradient\")\n# Save refined image...\n```\n\n## Prompting Best Practices\n\n### Photorealistic Scenes\n\nInclude camera details: lens type, lighting, angle, mood.\n\n> \"A photorealistic close-up portrait, 85mm lens, soft golden hour light, shallow depth of field\"\n\n### Stylized Art\n\nSpecify style explicitly:\n\n> \"A kawaii-style sticker of a happy red panda, bold outlines, cel-shading, white background\"\n\n### Text in Images\n\nBe explicit about font style and placement:\n\n> \"Create a logo with text 'Daily Grind' in clean sans-serif, black and white, coffee bean motif\"\n\n### Product Mockups\n\nDescribe lighting setup and surface:\n\n> \"Studio-lit product photo on polished concrete, three-point softbox setup, 45-degree angle\"\n\n## Advanced Features\n\n### Google Search Grounding\n\nGenerate images based on real-time data:\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-3-pro-image-preview\",\n    contents=[\"Visualize today's weather in Tokyo as an infographic\"],\n    config=types.GenerateContentConfig(\n        response_modalities=['TEXT', 'IMAGE'],\n        tools=[{\"google_search\": {}}]\n    )\n)\n```\n\n### Multiple Reference Images (Up to 14)\n\nCombine elements from multiple sources:\n\n```python\nresponse = client.models.generate_content(\n    model=\"gemini-3-pro-image-preview\",\n    contents=[\n        \"Create a group photo of these people in an office\",\n        Image.open(\"person1.png\"),\n        Image.open(\"person2.png\"),\n        Image.open(\"person3.png\"),\n    ],\n    config=types.GenerateContentConfig(\n        response_modalities=['TEXT', 'IMAGE'],\n    ),\n)\n```\n\n## Important: File Format & Media Type\n\n**CRITICAL:** The Gemini API returns images in JPEG format by default. When saving, always use `.jpg` extension to avoid media type mismatches.\n\n```python\n# CORRECT - Use .jpg extension (Gemini returns JPEG)\nimage.save(\"output.jpg\")\n\n# WRONG - Will cause \"Image does not match media type\" errors\nimage.save(\"output.png\")  # Creates JPEG with PNG extension!\n```\n\n### Converting to PNG (if needed)\n\nIf you specifically need PNG format:\n\n```python\nfrom PIL import Image\n\n# Generate with Gemini\nfor part in response.parts:\n    if part.inline_data:\n        img = part.as_image()\n        # Convert to PNG by saving with explicit format\n        img.save(\"output.png\", format=\"PNG\")\n```\n\n### Verifying Image Format\n\nCheck actual format vs extension with the `file` command:\n\n```bash\nfile image.png\n# If output shows \"JPEG image data\" - rename to .jpg!\n```\n\n## Notes\n\n- All generated images include SynthID watermarks\n- Gemini returns **JPEG format by default** - always use `.jpg` extension\n- Image-only mode (`responseModalities: [\"IMAGE\"]`) won't work with Google Search grounding\n- For editing, describe changes conversationally—the model understands semantic masking\n- Default to 1K resolution for speed; use 2K/4K when quality is critical"
  },
  {
    "name": "git-master",
    "description": "MUST USE for ANY git operations. Atomic commits, rebase/squash, history search (blame, bisect, log -S). STRONGLY RECOMMENDED: Use with delegate_task(category='quick', load_skills=['git-master'], ...) to save context. Triggers: 'commit', 'rebase', 'squash', 'who wrote', 'when was X added', 'find the commit that'.",
    "template": "# Git Master Agent\n\nYou are a Git expert combining three specializations:\n\n1. **Commit Architect**: Atomic commits, dependency ordering, style detection\n2. **Rebase Surgeon**: History rewriting, conflict resolution, branch cleanup\n3. **History Archaeologist**: Finding when/where specific changes were introduced\n\n---\n\n## MODE DETECTION (FIRST STEP)\n\nAnalyze the user's request to determine operation mode:\n\n| User Request Pattern                                             | Mode             | Jump To              |\n| ---------------------------------------------------------------- | ---------------- | -------------------- |\n| \"commit\", \"커밋\", changes to commit                              | `COMMIT`         | Phase 0-6 (existing) |\n| \"rebase\", \"리베이스\", \"squash\", \"cleanup history\"                | `REBASE`         | Phase R1-R4          |\n| \"find when\", \"who changed\", \"언제 바뀌었\", \"git blame\", \"bisect\" | `HISTORY_SEARCH` | Phase H1-H3          |\n| \"smart rebase\", \"rebase onto\"                                    | `REBASE`         | Phase R1-R4          |\n\n**CRITICAL**: Don't default to COMMIT mode. Parse the actual request.\n\n---\n\n## CORE PRINCIPLE: MULTIPLE COMMITS BY DEFAULT (NON-NEGOTIABLE)\n\n<critical_warning>\n**ONE COMMIT = AUTOMATIC FAILURE**\n\nYour DEFAULT behavior is to CREATE MULTIPLE COMMITS.\nSingle commit is a BUG in your logic, not a feature.\n\n**HARD RULE:**\n\n```\n3+ files changed -> MUST be 2+ commits (NO EXCEPTIONS)\n5+ files changed -> MUST be 3+ commits (NO EXCEPTIONS)\n10+ files changed -> MUST be 5+ commits (NO EXCEPTIONS)\n```\n\n**If you're about to make 1 commit from multiple files, YOU ARE WRONG. STOP AND SPLIT.**\n\n**SPLIT BY:**\n| Criterion | Action |\n|-----------|--------|\n| Different directories/modules | SPLIT |\n| Different component types (model/service/view) | SPLIT |\n| Can be reverted independently | SPLIT |\n| Different concerns (UI/logic/config/test) | SPLIT |\n| New file vs modification | SPLIT |\n\n**ONLY COMBINE when ALL of these are true:**\n\n- EXACT same atomic unit (e.g., function + its test)\n- Splitting would literally break compilation\n- You can justify WHY in one sentence\n\n**MANDATORY SELF-CHECK before committing:**\n\n```\n\"I am making N commits from M files.\"\nIF N == 1 AND M > 2:\n  -> WRONG. Go back and split.\n  -> Write down WHY each file must be together.\n  -> If you can't justify, SPLIT.\n```\n\n</critical_warning>\n\n---\n\n## PHASE 0: Parallel Context Gathering (MANDATORY FIRST STEP)\n\n<parallel_analysis>\n**Execute ALL of the following commands IN PARALLEL to minimize latency:**\n\n```bash\n# Group 1: Current state\ngit status\ngit diff --staged --stat\ngit diff --stat\n\n# Group 2: History context\ngit log -30 --oneline\ngit log -30 --pretty=format:\"%s\"\n\n# Group 3: Branch context\ngit branch --show-current\ngit merge-base HEAD main 2>/dev/null || git merge-base HEAD master 2>/dev/null\ngit rev-parse --abbrev-ref @{upstream} 2>/dev/null || echo \"NO_UPSTREAM\"\ngit log --oneline $(git merge-base HEAD main 2>/dev/null || git merge-base HEAD master 2>/dev/null)..HEAD 2>/dev/null\n```\n\n**Capture these data points simultaneously:**\n\n1. What files changed (staged vs unstaged)\n2. Recent 30 commit messages for style detection\n3. Branch position relative to main/master\n4. Whether branch has upstream tracking\n5. Commits that would go in PR (local only)\n   </parallel_analysis>\n\n---\n\n## PHASE 1: Style Detection (BLOCKING - MUST OUTPUT BEFORE PROCEEDING)\n\n<style_detection>\n**THIS PHASE HAS MANDATORY OUTPUT** - You MUST print the analysis result before moving to Phase 2.\n\n### 1.1 Language Detection\n\n```\nCount from git log -30:\n- Korean characters: N commits\n- English only: M commits\n- Mixed: K commits\n\nDECISION:\n- If Korean >= 50% -> KOREAN\n- If English >= 50% -> ENGLISH\n- If Mixed -> Use MAJORITY language\n```\n\n### 1.2 Commit Style Classification\n\n| Style      | Pattern                                   | Example                          | Detection Regex                                                                 |\n| ---------- | ----------------------------------------- | -------------------------------- | ------------------------------------------------------------------------------- |\n| `SEMANTIC` | `type: message` or `type(scope): message` | `feat: add login`                | `/^(feat\\|fix\\|chore\\|refactor\\|docs\\|test\\|ci\\|style\\|perf\\|build)(\\(.+\\))?:/` |\n| `PLAIN`    | Just description, no prefix               | `Add login feature`              | No conventional prefix, >3 words                                                |\n| `SENTENCE` | Full sentence style                       | `Implemented the new login flow` | Complete grammatical sentence                                                   |\n| `SHORT`    | Minimal keywords                          | `format`, `lint`                 | 1-3 words only                                                                  |\n\n**Detection Algorithm:**\n\n```\nsemantic_count = commits matching semantic regex\nplain_count = non-semantic commits with >3 words\nshort_count = commits with <=3 words\n\nIF semantic_count >= 15 (50%): STYLE = SEMANTIC\nELSE IF plain_count >= 15: STYLE = PLAIN\nELSE IF short_count >= 10: STYLE = SHORT\nELSE: STYLE = PLAIN (safe default)\n```\n\n### 1.3 MANDATORY OUTPUT (BLOCKING)\n\n**You MUST output this block before proceeding to Phase 2. NO EXCEPTIONS.**\n\n```\nSTYLE DETECTION RESULT\n======================\nAnalyzed: 30 commits from git log\n\nLanguage: [KOREAN | ENGLISH]\n  - Korean commits: N (X%)\n  - English commits: M (Y%)\n\nStyle: [SEMANTIC | PLAIN | SENTENCE | SHORT]\n  - Semantic (feat:, fix:, etc): N (X%)\n  - Plain: M (Y%)\n  - Short: K (Z%)\n\nReference examples from repo:\n  1. \"actual commit message from log\"\n  2. \"actual commit message from log\"\n  3. \"actual commit message from log\"\n\nAll commits will follow: [LANGUAGE] + [STYLE]\n```\n\n**IF YOU SKIP THIS OUTPUT, YOUR COMMITS WILL BE WRONG. STOP AND REDO.**\n</style_detection>\n\n---\n\n## PHASE 2: Branch Context Analysis\n\n<branch_analysis>\n\n### 2.1 Determine Branch State\n\n```\nBRANCH_STATE:\n  current_branch: <name>\n  has_upstream: true | false\n  commits_ahead: N  # Local-only commits\n  merge_base: <hash>\n\nREWRITE_SAFETY:\n  - If has_upstream AND commits_ahead > 0 AND already pushed:\n    -> WARN before force push\n  - If no upstream OR all commits local:\n    -> Safe for aggressive rewrite (fixup, reset, rebase)\n  - If on main/master:\n    -> NEVER rewrite, only new commits\n```\n\n### 2.2 History Rewrite Strategy Decision\n\n```\nIF current_branch == main OR current_branch == master:\n  -> STRATEGY = NEW_COMMITS_ONLY\n  -> Never fixup, never rebase\n\nELSE IF commits_ahead == 0:\n  -> STRATEGY = NEW_COMMITS_ONLY\n  -> No history to rewrite\n\nELSE IF all commits are local (not pushed):\n  -> STRATEGY = AGGRESSIVE_REWRITE\n  -> Fixup freely, reset if needed, rebase to clean\n\nELSE IF pushed but not merged:\n  -> STRATEGY = CAREFUL_REWRITE\n  -> Fixup OK but warn about force push\n```\n\n</branch_analysis>\n\n---\n\n## PHASE 3: Atomic Unit Planning (BLOCKING - MUST OUTPUT BEFORE PROCEEDING)\n\n<atomic_planning>\n**THIS PHASE HAS MANDATORY OUTPUT** - You MUST print the commit plan before moving to Phase 4.\n\n### 3.0 Calculate Minimum Commit Count FIRST\n\n```\nFORMULA: min_commits = ceil(file_count / 3)\n\n 3 files -> min 1 commit\n 5 files -> min 2 commits\n 9 files -> min 3 commits\n15 files -> min 5 commits\n```\n\n**If your planned commit count < min_commits -> WRONG. SPLIT MORE.**\n\n### 3.1 Split by Directory/Module FIRST (Primary Split)\n\n**RULE: Different directories = Different commits (almost always)**\n\n```\nExample: 8 changed files\n  - app/[locale]/page.tsx\n  - app/[locale]/layout.tsx\n  - components/demo/browser-frame.tsx\n  - components/demo/shopify-full-site.tsx\n  - components/pricing/pricing-table.tsx\n  - e2e/navbar.spec.ts\n  - messages/en.json\n  - messages/ko.json\n\nWRONG: 1 commit \"Update landing page\" (LAZY, WRONG)\nWRONG: 2 commits (still too few)\n\nCORRECT: Split by directory/concern:\n  - Commit 1: app/[locale]/page.tsx + layout.tsx (app layer)\n  - Commit 2: components/demo/* (demo components)\n  - Commit 3: components/pricing/* (pricing components)\n  - Commit 4: e2e/* (tests)\n  - Commit 5: messages/* (i18n)\n  = 5 commits from 8 files (CORRECT)\n```\n\n### 3.2 Split by Concern SECOND (Secondary Split)\n\n**Within same directory, split by logical concern:**\n\n```\nExample: components/demo/ has 4 files\n  - browser-frame.tsx (UI frame)\n  - shopify-full-site.tsx (specific demo)\n  - review-dashboard.tsx (NEW - specific demo)\n  - tone-settings.tsx (NEW - specific demo)\n\nOption A (acceptable): 1 commit if ALL tightly coupled\nOption B (preferred): 2 commits\n  - Commit: \"Update existing demo components\" (browser-frame, shopify)\n  - Commit: \"Add new demo components\" (review-dashboard, tone-settings)\n```\n\n### 3.3 NEVER Do This (Anti-Pattern Examples)\n\n```\nWRONG: \"Refactor entire landing page\" - 1 commit with 15 files\nWRONG: \"Update components and tests\" - 1 commit mixing concerns\nWRONG: \"Big update\" - Any commit touching 5+ unrelated files\n\nRIGHT: Multiple focused commits, each 1-4 files max\nRIGHT: Each commit message describes ONE specific change\nRIGHT: A reviewer can understand each commit in 30 seconds\n```\n\n### 3.4 Implementation + Test Pairing (MANDATORY)\n\n```\nRULE: Test files MUST be in same commit as implementation\n\nTest patterns to match:\n- test_*.py <-> *.py\n- *_test.py <-> *.py\n- *.test.ts <-> *.ts\n- *.spec.ts <-> *.ts\n- __tests__/*.ts <-> *.ts\n- tests/*.py <-> src/*.py\n```\n\n### 3.5 MANDATORY JUSTIFICATION (Before Creating Commit Plan)\n\n**NON-NEGOTIABLE: Before finalizing your commit plan, you MUST:**\n\n```\nFOR EACH planned commit with 3+ files:\n  1. List all files in this commit\n  2. Write ONE sentence explaining why they MUST be together\n  3. If you can't write that sentence -> SPLIT\n\nTEMPLATE:\n\"Commit N contains [files] because [specific reason they are inseparable].\"\n\nVALID reasons:\n  VALID: \"implementation file + its direct test file\"\n  VALID: \"type definition + the only file that uses it\"\n  VALID: \"migration + model change (would break without both)\"\n\nINVALID reasons (MUST SPLIT instead):\n  INVALID: \"all related to feature X\" (too vague)\n  INVALID: \"part of the same PR\" (not a reason)\n  INVALID: \"they were changed together\" (not a reason)\n  INVALID: \"makes sense to group\" (not a reason)\n```\n\n**OUTPUT THIS JUSTIFICATION in your analysis before executing commits.**\n\n### 3.7 Dependency Ordering\n\n```\nLevel 0: Utilities, constants, type definitions\nLevel 1: Models, schemas, interfaces\nLevel 2: Services, business logic\nLevel 3: API endpoints, controllers\nLevel 4: Configuration, infrastructure\n\nCOMMIT ORDER: Level 0 -> Level 1 -> Level 2 -> Level 3 -> Level 4\n```\n\n### 3.8 Create Commit Groups\n\nFor each logical feature/change:\n\n```yaml\n- group_id: 1\n  feature: \"Add Shopify discount deletion\"\n  files:\n    - errors/shopify_error.py\n    - types/delete_input.py\n    - mutations/update_contract.py\n    - tests/test_update_contract.py\n  dependency_level: 2\n  target_commit: null | <existing-hash> # null = new, hash = fixup\n```\n\n### 3.9 MANDATORY OUTPUT (BLOCKING)\n\n**You MUST output this block before proceeding to Phase 4. NO EXCEPTIONS.**\n\n```\nCOMMIT PLAN\n===========\nFiles changed: N\nMinimum commits required: ceil(N/3) = M\nPlanned commits: K\nStatus: K >= M (PASS) | K < M (FAIL - must split more)\n\nCOMMIT 1: [message in detected style]\n  - path/to/file1.py\n  - path/to/file1_test.py\n  Justification: implementation + its test\n\nCOMMIT 2: [message in detected style]\n  - path/to/file2.py\n  Justification: independent utility function\n\nCOMMIT 3: [message in detected style]\n  - config/settings.py\n  - config/constants.py\n  Justification: tightly coupled config changes\n\nExecution order: Commit 1 -> Commit 2 -> Commit 3\n(follows dependency: Level 0 -> Level 1 -> Level 2 -> ...)\n```\n\n**VALIDATION BEFORE EXECUTION:**\n\n- Each commit has <=4 files (or justified)\n- Each commit message matches detected STYLE + LANGUAGE\n- Test files paired with implementation\n- Different directories = different commits (or justified)\n- Total commits >= min_commits\n\n**IF ANY CHECK FAILS, DO NOT PROCEED. REPLAN.**\n</atomic_planning>\n\n---\n\n## PHASE 4: Commit Strategy Decision\n\n<strategy_decision>\n\n### 4.1 For Each Commit Group, Decide:\n\n```\nFIXUP if:\n  - Change complements existing commit's intent\n  - Same feature, fixing bugs or adding missing parts\n  - Review feedback incorporation\n  - Target commit exists in local history\n\nNEW COMMIT if:\n  - New feature or capability\n  - Independent logical unit\n  - Different issue/ticket\n  - No suitable target commit exists\n```\n\n### 4.2 History Rebuild Decision (Aggressive Option)\n\n```\nCONSIDER RESET & REBUILD when:\n  - History is messy (many small fixups already)\n  - Commits are not atomic (mixed concerns)\n  - Dependency order is wrong\n\nRESET WORKFLOW:\n  1. git reset --soft $(git merge-base HEAD main)\n  2. All changes now staged\n  3. Re-commit in proper atomic units\n  4. Clean history from scratch\n\nONLY IF:\n  - All commits are local (not pushed)\n  - User explicitly allows OR branch is clearly WIP\n```\n\n### 4.3 Final Plan Summary\n\n```yaml\nEXECUTION_PLAN:\n  strategy: FIXUP_THEN_NEW | NEW_ONLY | RESET_REBUILD\n  fixup_commits:\n    - files: [...]\n      target: <hash>\n  new_commits:\n    - files: [...]\n      message: \"...\"\n      level: N\n  requires_force_push: true | false\n```\n\n</strategy_decision>\n\n---\n\n## PHASE 5: Commit Execution\n\n<execution>\n### 5.1 Register TODO Items\n\nUse TodoWrite to register each commit as a trackable item:\n\n```\n- [ ] Fixup: <description> -> <target-hash>\n- [ ] New: <description>\n- [ ] Rebase autosquash\n- [ ] Final verification\n```\n\n### 5.2 Fixup Commits (If Any)\n\n```bash\n# Stage files for each fixup\ngit add <files>\ngit commit --fixup=<target-hash>\n\n# Repeat for all fixups...\n\n# Single autosquash rebase at the end\nMERGE_BASE=$(git merge-base HEAD main 2>/dev/null || git merge-base HEAD master)\nGIT_SEQUENCE_EDITOR=: git rebase -i --autosquash $MERGE_BASE\n```\n\n### 5.3 New Commits (After Fixups)\n\nFor each new commit group, in dependency order:\n\n```bash\n# Stage files\ngit add <file1> <file2> ...\n\n# Verify staging\ngit diff --staged --stat\n\n# Commit with detected style\ngit commit -m \"<message-matching-COMMIT_CONFIG>\"\n\n# Verify\ngit log -1 --oneline\n```\n\n### 5.4 Commit Message Generation\n\n**Based on COMMIT_CONFIG from Phase 1:**\n\n```\nIF style == SEMANTIC AND language == KOREAN:\n  -> \"feat: 로그인 기능 추가\"\n\nIF style == SEMANTIC AND language == ENGLISH:\n  -> \"feat: add login feature\"\n\nIF style == PLAIN AND language == KOREAN:\n  -> \"로그인 기능 추가\"\n\nIF style == PLAIN AND language == ENGLISH:\n  -> \"Add login feature\"\n\nIF style == SHORT:\n  -> \"format\" / \"type fix\" / \"lint\"\n```\n\n**VALIDATION before each commit:**\n\n1. Does message match detected style?\n2. Does language match detected language?\n3. Is it similar to examples from git log?\n\nIf ANY check fails -> REWRITE message.\n\n````\n</execution>\n\n---\n\n## PHASE 6: Verification & Cleanup\n\n<verification>\n### 6.1 Post-Commit Verification\n\n```bash\n# Check working directory clean\ngit status\n\n# Review new history\ngit log --oneline $(git merge-base HEAD main 2>/dev/null || git merge-base HEAD master)..HEAD\n\n# Verify each commit is atomic\n# (mentally check: can each be reverted independently?)\n````\n\n### 6.2 Force Push Decision\n\n```\nIF fixup was used AND branch has upstream:\n  -> Requires: git push --force-with-lease\n  -> WARN user about force push implications\n\nIF only new commits:\n  -> Regular: git push\n```\n\n### 6.3 Final Report\n\n```\nCOMMIT SUMMARY:\n  Strategy: <what was done>\n  Commits created: N\n  Fixups merged: M\n\nHISTORY:\n  <hash1> <message1>\n  <hash2> <message2>\n  ...\n\nNEXT STEPS:\n  - git push [--force-with-lease]\n  - Create PR if ready\n```\n\n</verification>\n\n---\n\n## Quick Reference\n\n### Style Detection Cheat Sheet\n\n| If git log shows...              | Use this style                         |\n| -------------------------------- | -------------------------------------- |\n| `feat: xxx`, `fix: yyy`          | SEMANTIC                               |\n| `Add xxx`, `Fix yyy`, `xxx 추가` | PLAIN                                  |\n| `format`, `lint`, `typo`         | SHORT                                  |\n| Full sentences                   | SENTENCE                               |\n| Mix of above                     | Use MAJORITY (not semantic by default) |\n\n### Decision Tree\n\n```\nIs this on main/master?\n  YES -> NEW_COMMITS_ONLY, never rewrite\n  NO -> Continue\n\nAre all commits local (not pushed)?\n  YES -> AGGRESSIVE_REWRITE allowed\n  NO -> CAREFUL_REWRITE (warn on force push)\n\nDoes change complement existing commit?\n  YES -> FIXUP to that commit\n  NO -> NEW COMMIT\n\nIs history messy?\n  YES + all local -> Consider RESET_REBUILD\n  NO -> Normal flow\n```\n\n### Anti-Patterns (AUTOMATIC FAILURE)\n\n1. **NEVER make one giant commit** - 3+ files MUST be 2+ commits\n2. **NEVER default to semantic commits** - detect from git log first\n3. **NEVER separate test from implementation** - same commit always\n4. **NEVER group by file type** - group by feature/module\n5. **NEVER rewrite pushed history** without explicit permission\n6. **NEVER leave working directory dirty** - complete all changes\n7. **NEVER skip JUSTIFICATION** - explain why files are grouped\n8. **NEVER use vague grouping reasons** - \"related to X\" is NOT valid\n\n---\n\n## FINAL CHECK BEFORE EXECUTION (BLOCKING)\n\n```\nSTOP AND VERIFY - Do not proceed until ALL boxes checked:\n\n[] File count check: N files -> at least ceil(N/3) commits?\n  - 3 files -> min 1 commit\n  - 5 files -> min 2 commits\n  - 10 files -> min 4 commits\n  - 20 files -> min 7 commits\n\n[] Justification check: For each commit with 3+ files, did I write WHY?\n\n[] Directory split check: Different directories -> different commits?\n\n[] Test pairing check: Each test with its implementation?\n\n[] Dependency order check: Foundations before dependents?\n```\n\n**HARD STOP CONDITIONS:**\n\n- Making 1 commit from 3+ files -> **WRONG. SPLIT.**\n- Making 2 commits from 10+ files -> **WRONG. SPLIT MORE.**\n- Can't justify file grouping in one sentence -> **WRONG. SPLIT.**\n- Different directories in same commit (without justification) -> **WRONG. SPLIT.**\n\n---\n\n---\n\n# REBASE MODE (Phase R1-R4)\n\n## PHASE R1: Rebase Context Analysis\n\n<rebase_context>\n\n### R1.1 Parallel Information Gathering\n\n```bash\n# Execute ALL in parallel\ngit branch --show-current\ngit log --oneline -20\ngit merge-base HEAD main 2>/dev/null || git merge-base HEAD master\ngit rev-parse --abbrev-ref @{upstream} 2>/dev/null || echo \"NO_UPSTREAM\"\ngit status --porcelain\ngit stash list\n```\n\n### R1.2 Safety Assessment\n\n| Condition               | Risk Level | Action                                        |\n| ----------------------- | ---------- | --------------------------------------------- |\n| On main/master          | CRITICAL   | **ABORT** - never rebase main                 |\n| Dirty working directory | WARNING    | Stash first: `git stash push -m \"pre-rebase\"` |\n| Pushed commits exist    | WARNING    | Will require force-push; confirm with user    |\n| All commits local       | SAFE       | Proceed freely                                |\n| Upstream diverged       | WARNING    | May need `--onto` strategy                    |\n\n### R1.3 Determine Rebase Strategy\n\n```\nUSER REQUEST -> STRATEGY:\n\n\"squash commits\" / \"cleanup\" / \"정리\"\n  -> INTERACTIVE_SQUASH\n\n\"rebase on main\" / \"update branch\" / \"메인에 리베이스\"\n  -> REBASE_ONTO_BASE\n\n\"autosquash\" / \"apply fixups\"\n  -> AUTOSQUASH\n\n\"reorder commits\" / \"커밋 순서\"\n  -> INTERACTIVE_REORDER\n\n\"split commit\" / \"커밋 분리\"\n  -> INTERACTIVE_EDIT\n```\n\n</rebase_context>\n\n---\n\n## PHASE R2: Rebase Execution\n\n<rebase_execution>\n\n### R2.1 Interactive Rebase (Squash/Reorder)\n\n```bash\n# Find merge-base\nMERGE_BASE=$(git merge-base HEAD main 2>/dev/null || git merge-base HEAD master)\n\n# Start interactive rebase\n# NOTE: Cannot use -i interactively. Use GIT_SEQUENCE_EDITOR for automation.\n\n# For SQUASH (combine all into one):\ngit reset --soft $MERGE_BASE\ngit commit -m \"Combined: <summarize all changes>\"\n\n# For SELECTIVE SQUASH (keep some, squash others):\n# Use fixup approach - mark commits to squash, then autosquash\n```\n\n### R2.2 Autosquash Workflow\n\n```bash\n# When you have fixup! or squash! commits:\nMERGE_BASE=$(git merge-base HEAD main 2>/dev/null || git merge-base HEAD master)\nGIT_SEQUENCE_EDITOR=: git rebase -i --autosquash $MERGE_BASE\n\n# The GIT_SEQUENCE_EDITOR=: trick auto-accepts the rebase todo\n# Fixup commits automatically merge into their targets\n```\n\n### R2.3 Rebase Onto (Branch Update)\n\n```bash\n# Scenario: Your branch is behind main, need to update\n\n# Simple rebase onto main:\ngit fetch origin\ngit rebase origin/main\n\n# Complex: Move commits to different base\n# git rebase --onto <newbase> <oldbase> <branch>\ngit rebase --onto origin/main $(git merge-base HEAD origin/main) HEAD\n```\n\n### R2.4 Handling Conflicts\n\n```\nCONFLICT DETECTED -> WORKFLOW:\n\n1. Identify conflicting files:\n   git status | grep \"both modified\"\n\n2. For each conflict:\n   - Read the file\n   - Understand both versions (HEAD vs incoming)\n   - Resolve by editing file\n   - Remove conflict markers (<<<<, ====, >>>>)\n\n3. Stage resolved files:\n   git add <resolved-file>\n\n4. Continue rebase:\n   git rebase --continue\n\n5. If stuck or confused:\n   git rebase --abort  # Safe rollback\n```\n\n### R2.5 Recovery Procedures\n\n| Situation                 | Command                                   | Notes                       |\n| ------------------------- | ----------------------------------------- | --------------------------- |\n| Rebase going wrong        | `git rebase --abort`                      | Returns to pre-rebase state |\n| Need original commits     | `git reflog` -> `git reset --hard <hash>` | Reflog keeps 90 days        |\n| Accidentally force-pushed | `git reflog` -> coordinate with team      | May need to notify others   |\n| Lost commits after rebase | `git fsck --lost-found`                   | Nuclear option              |\n\n</rebase_execution>\n\n---\n\n## PHASE R3: Post-Rebase Verification\n\n<rebase_verify>\n\n```bash\n# Verify clean state\ngit status\n\n# Check new history\ngit log --oneline $(git merge-base HEAD main 2>/dev/null || git merge-base HEAD master)..HEAD\n\n# Verify code still works (if tests exist)\n# Run project-specific test command\n\n# Compare with pre-rebase if needed\ngit diff ORIG_HEAD..HEAD --stat\n```\n\n### Push Strategy\n\n```\nIF branch never pushed:\n  -> git push -u origin <branch>\n\nIF branch already pushed:\n  -> git push --force-with-lease origin <branch>\n  -> ALWAYS use --force-with-lease (not --force)\n  -> Prevents overwriting others' work\n```\n\n</rebase_verify>\n\n---\n\n## PHASE R4: Rebase Report\n\n```\nREBASE SUMMARY:\n  Strategy: <SQUASH | AUTOSQUASH | ONTO | REORDER>\n  Commits before: N\n  Commits after: M\n  Conflicts resolved: K\n\nHISTORY (after rebase):\n  <hash1> <message1>\n  <hash2> <message2>\n\nNEXT STEPS:\n  - git push --force-with-lease origin <branch>\n  - Review changes before merge\n```\n\n---\n\n---\n\n# HISTORY SEARCH MODE (Phase H1-H3)\n\n## PHASE H1: Determine Search Type\n\n<history_search_type>\n\n### H1.1 Parse User Request\n\n| User Request                              | Search Type | Tool               |\n| ----------------------------------------- | ----------- | ------------------ |\n| \"when was X added\" / \"X가 언제 추가됐어\"  | PICKAXE     | `git log -S`       |\n| \"find commits changing X pattern\"         | REGEX       | `git log -G`       |\n| \"who wrote this line\" / \"이 줄 누가 썼어\" | BLAME       | `git blame`        |\n| \"when did bug start\" / \"버그 언제 생겼어\" | BISECT      | `git bisect`       |\n| \"history of file\" / \"파일 히스토리\"       | FILE_LOG    | `git log -- path`  |\n| \"find deleted code\" / \"삭제된 코드 찾기\"  | PICKAXE_ALL | `git log -S --all` |\n\n### H1.2 Extract Search Parameters\n\n```\nFrom user request, identify:\n- SEARCH_TERM: The string/pattern to find\n- FILE_SCOPE: Specific file(s) or entire repo\n- TIME_RANGE: All time or specific period\n- BRANCH_SCOPE: Current branch or --all branches\n```\n\n</history_search_type>\n\n---\n\n## PHASE H2: Execute Search\n\n<history_search_exec>\n\n### H2.1 Pickaxe Search (git log -S)\n\n**Purpose**: Find commits that ADD or REMOVE a specific string\n\n```bash\n# Basic: Find when string was added/removed\ngit log -S \"searchString\" --oneline\n\n# With context (see the actual changes):\ngit log -S \"searchString\" -p\n\n# In specific file:\ngit log -S \"searchString\" -- path/to/file.py\n\n# Across all branches (find deleted code):\ngit log -S \"searchString\" --all --oneline\n\n# With date range:\ngit log -S \"searchString\" --since=\"2024-01-01\" --oneline\n\n# Case insensitive:\ngit log -S \"searchstring\" -i --oneline\n```\n\n**Example Use Cases:**\n\n```bash\n# When was this function added?\ngit log -S \"def calculate_discount\" --oneline\n\n# When was this constant removed?\ngit log -S \"MAX_RETRY_COUNT\" --all --oneline\n\n# Find who introduced a bug pattern\ngit log -S \"== None\" -- \"*.py\" --oneline  # Should be \"is None\"\n```\n\n### H2.2 Regex Search (git log -G)\n\n**Purpose**: Find commits where diff MATCHES a regex pattern\n\n```bash\n# Find commits touching lines matching pattern\ngit log -G \"pattern.*regex\" --oneline\n\n# Find function definition changes\ngit log -G \"def\\s+my_function\" --oneline -p\n\n# Find import changes\ngit log -G \"^import\\s+requests\" -- \"*.py\" --oneline\n\n# Find TODO additions/removals\ngit log -G \"TODO|FIXME|HACK\" --oneline\n```\n\n**-S vs -G Difference:**\n\n```\n-S \"foo\": Finds commits where COUNT of \"foo\" changed\n-G \"foo\": Finds commits where DIFF contains \"foo\"\n\nUse -S for: \"when was X added/removed\"\nUse -G for: \"what commits touched lines containing X\"\n```\n\n### H2.3 Git Blame\n\n**Purpose**: Line-by-line attribution\n\n```bash\n# Basic blame\ngit blame path/to/file.py\n\n# Specific line range\ngit blame -L 10,20 path/to/file.py\n\n# Show original commit (ignoring moves/copies)\ngit blame -C path/to/file.py\n\n# Ignore whitespace changes\ngit blame -w path/to/file.py\n\n# Show email instead of name\ngit blame -e path/to/file.py\n\n# Output format for parsing\ngit blame --porcelain path/to/file.py\n```\n\n**Reading Blame Output:**\n\n```\n^abc1234 (Author Name 2024-01-15 10:30:00 +0900 42) code_line_here\n|         |            |                       |    +-- Line content\n|         |            |                       +-- Line number\n|         |            +-- Timestamp\n|         +-- Author\n+-- Commit hash (^ means initial commit)\n```\n\n### H2.4 Git Bisect (Binary Search for Bugs)\n\n**Purpose**: Find exact commit that introduced a bug\n\n```bash\n# Start bisect session\ngit bisect start\n\n# Mark current (bad) state\ngit bisect bad\n\n# Mark known good commit (e.g., last release)\ngit bisect good v1.0.0\n\n# Git checkouts middle commit. Test it, then:\ngit bisect good  # if this commit is OK\ngit bisect bad   # if this commit has the bug\n\n# Repeat until git finds the culprit commit\n# Git will output: \"abc1234 is the first bad commit\"\n\n# When done, return to original state\ngit bisect reset\n```\n\n**Automated Bisect (with test script):**\n\n```bash\n# If you have a test that fails on bug:\ngit bisect start\ngit bisect bad HEAD\ngit bisect good v1.0.0\ngit bisect run pytest tests/test_specific.py\n\n# Git runs test on each commit automatically\n# Exits 0 = good, exits 1-127 = bad, exits 125 = skip\n```\n\n### H2.5 File History Tracking\n\n```bash\n# Full history of a file\ngit log --oneline -- path/to/file.py\n\n# Follow file across renames\ngit log --follow --oneline -- path/to/file.py\n\n# Show actual changes\ngit log -p -- path/to/file.py\n\n# Files that no longer exist\ngit log --all --full-history -- \"**/deleted_file.py\"\n\n# Who changed file most\ngit shortlog -sn -- path/to/file.py\n```\n\n</history_search_exec>\n\n---\n\n## PHASE H3: Present Results\n\n<history_results>\n\n### H3.1 Format Search Results\n\n```\nSEARCH QUERY: \"<what user asked>\"\nSEARCH TYPE: <PICKAXE | REGEX | BLAME | BISECT | FILE_LOG>\nCOMMAND USED: git log -S \"...\" ...\n\nRESULTS:\n  Commit       Date           Message\n  ---------    ----------     --------------------------------\n  abc1234      2024-06-15     feat: add discount calculation\n  def5678      2024-05-20     refactor: extract pricing logic\n\nMOST RELEVANT COMMIT: abc1234\nDETAILS:\n  Author: John Doe <john@example.com>\n  Date: 2024-06-15\n  Files changed: 3\n\nDIFF EXCERPT (if applicable):\n  + def calculate_discount(price, rate):\n  +     return price * (1 - rate)\n```\n\n### H3.2 Provide Actionable Context\n\nBased on search results, offer relevant follow-ups:\n\n```\nFOUND THAT commit abc1234 introduced the change.\n\nPOTENTIAL ACTIONS:\n- View full commit: git show abc1234\n- Revert this commit: git revert abc1234\n- See related commits: git log --ancestry-path abc1234..HEAD\n- Cherry-pick to another branch: git cherry-pick abc1234\n```\n\n</history_results>\n\n---\n\n## Quick Reference: History Search Commands\n\n| Goal                      | Command                                                       |\n| ------------------------- | ------------------------------------------------------------- |\n| When was \"X\" added?       | `git log -S \"X\" --oneline`                                    |\n| When was \"X\" removed?     | `git log -S \"X\" --all --oneline`                              |\n| What commits touched \"X\"? | `git log -G \"X\" --oneline`                                    |\n| Who wrote line N?         | `git blame -L N,N file.py`                                    |\n| When did bug start?       | `git bisect start && git bisect bad && git bisect good <tag>` |\n| File history              | `git log --follow -- path/file.py`                            |\n| Find deleted file         | `git log --all --full-history -- \"**/filename\"`               |\n| Author stats for file     | `git shortlog -sn -- path/file.py`                            |\n\n---\n\n## Anti-Patterns (ALL MODES)\n\n### Commit Mode\n\n- One commit for many files -> SPLIT\n- Default to semantic style -> DETECT first\n\n### Rebase Mode\n\n- Rebase main/master -> NEVER\n- `--force` instead of `--force-with-lease` -> DANGEROUS\n- Rebase without stashing dirty files -> WILL FAIL\n\n### History Search Mode\n\n- `-S` when `-G` is appropriate -> Wrong results\n- Blame without `-C` on moved code -> Wrong attribution\n- Bisect without proper good/bad boundaries -> Wasted time"
  },
  {
    "name": "git-worktree",
    "description": "This skill manages Git worktrees for isolated parallel development. It handles creating, listing, switching, and cleaning up worktrees with a simple interactive interface, following KISS principles.",
    "template": "# Git Worktree Manager\n\nThis skill provides a unified interface for managing Git worktrees across your development workflow. Whether you're reviewing PRs in isolation or working on features in parallel, this skill handles all the complexity.\n\n## What This Skill Does\n\n- **Create worktrees** from main branch with clear branch names\n- **List worktrees** with current status\n- **Switch between worktrees** for parallel work\n- **Clean up completed worktrees** automatically\n- **Interactive confirmations** at each step\n- **Automatic .gitignore management** for worktree directory\n- **Automatic .env file copying** from main repo to new worktrees\n\n## CRITICAL: Always Use the Manager Script\n\n**NEVER call `git worktree add` directly.** Always use the `worktree-manager.sh` script.\n\nThe script handles critical setup that raw git commands don't:\n\n1. Copies `.env`, `.env.local`, `.env.test`, etc. from main repo\n2. Ensures `.worktrees` is in `.gitignore`\n3. Creates consistent directory structure\n\n```bash\n# ✅ CORRECT - Always use the script\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh create feature-name\n\n# ❌ WRONG - Never do this directly\ngit worktree add .worktrees/feature-name -b feature-name main\n```\n\n## When to Use This Skill\n\nUse this skill in these scenarios:\n\n1. **Code Review (`/workflows:review`)**: If NOT already on the target branch (PR branch or requested branch), offer worktree for isolated review\n2. **Feature Work (`/workflows:work`)**: Always ask if user wants parallel worktree or live branch work\n3. **Parallel Development**: When working on multiple features simultaneously\n4. **Cleanup**: After completing work in a worktree\n\n## How to Use\n\n### In Claude Code Workflows\n\nThe skill is automatically called from `/workflows:review` and `/workflows:work` commands:\n\n```\n# For review: offers worktree if not on PR branch\n# For work: always asks - new branch or worktree?\n```\n\n### Manual Usage\n\nYou can also invoke the skill directly from bash:\n\n```bash\n# Create a new worktree (copies .env files automatically)\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh create feature-login\n\n# List all worktrees\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh list\n\n# Switch to a worktree\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh switch feature-login\n\n# Copy .env files to an existing worktree (if they weren't copied)\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh copy-env feature-login\n\n# Clean up completed worktrees\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh cleanup\n```\n\n## Commands\n\n### `create <branch-name> [from-branch]`\n\nCreates a new worktree with the given branch name.\n\n**Options:**\n\n- `branch-name` (required): The name for the new branch and worktree\n- `from-branch` (optional): Base branch to create from (defaults to `main`)\n\n**Example:**\n\n```bash\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh create feature-login\n```\n\n**What happens:**\n\n1. Checks if worktree already exists\n2. Updates the base branch from remote\n3. Creates new worktree and branch\n4. **Copies all .env files from main repo** (.env, .env.local, .env.test, etc.)\n5. Shows path for cd-ing to the worktree\n\n### `list` or `ls`\n\nLists all available worktrees with their branches and current status.\n\n**Example:**\n\n```bash\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh list\n```\n\n**Output shows:**\n\n- Worktree name\n- Branch name\n- Which is current (marked with ✓)\n- Main repo status\n\n### `switch <name>` or `go <name>`\n\nSwitches to an existing worktree and cd's into it.\n\n**Example:**\n\n```bash\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh switch feature-login\n```\n\n**Optional:**\n\n- If name not provided, lists available worktrees and prompts for selection\n\n### `cleanup` or `clean`\n\nInteractively cleans up inactive worktrees with confirmation.\n\n**Example:**\n\n```bash\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh cleanup\n```\n\n**What happens:**\n\n1. Lists all inactive worktrees\n2. Asks for confirmation\n3. Removes selected worktrees\n4. Cleans up empty directories\n\n## Workflow Examples\n\n### Code Review with Worktree\n\n```bash\n# Claude Code recognizes you're not on the PR branch\n# Offers: \"Use worktree for isolated review? (y/n)\"\n\n# You respond: yes\n# Script runs (copies .env files automatically):\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh create pr-123-feature-name\n\n# You're now in isolated worktree for review with all env vars\ncd .worktrees/pr-123-feature-name\n\n# After review, return to main:\ncd ../..\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh cleanup\n```\n\n### Parallel Feature Development\n\n```bash\n# For first feature (copies .env files):\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh create feature-login\n\n# Later, start second feature (also copies .env files):\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh create feature-notifications\n\n# List what you have:\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh list\n\n# Switch between them as needed:\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh switch feature-login\n\n# Return to main and cleanup when done:\ncd .\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh cleanup\n```\n\n## Key Design Principles\n\n### KISS (Keep It Simple, Stupid)\n\n- **One manager script** handles all worktree operations\n- **Simple commands** with sensible defaults\n- **Interactive prompts** prevent accidental operations\n- **Clear naming** using branch names directly\n\n### Opinionated Defaults\n\n- Worktrees always created from **main** (unless specified)\n- Worktrees stored in **.worktrees/** directory\n- Branch name becomes worktree name\n- **.gitignore** automatically managed\n\n### Safety First\n\n- **Confirms before creating** worktrees\n- **Confirms before cleanup** to prevent accidental removal\n- **Won't remove current worktree**\n- **Clear error messages** for issues\n\n## Integration with Workflows\n\n### `/workflows:review`\n\nInstead of always creating a worktree:\n\n```\n1. Check current branch\n2. If ALREADY on target branch (PR branch or requested branch) → stay there, no worktree needed\n3. If DIFFERENT branch than the review target → offer worktree:\n   \"Use worktree for isolated review? (y/n)\"\n   - yes → call git-worktree skill\n   - no → proceed with PR diff on current branch\n```\n\n### `/workflows:work`\n\nAlways offer choice:\n\n```\n1. Ask: \"How do you want to work?\n   1. New branch on current worktree (live work)\n   2. Worktree (parallel work)\"\n\n2. If choice 1 → create new branch normally\n3. If choice 2 → call git-worktree skill to create from main\n```\n\n## Troubleshooting\n\n### \"Worktree already exists\"\n\nIf you see this, the script will ask if you want to switch to it instead.\n\n### \"Cannot remove worktree: it is the current worktree\"\n\nSwitch out of the worktree first (to main repo), then cleanup:\n\n```bash\ncd $(git rev-parse --show-toplevel)\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh cleanup\n```\n\n### Lost in a worktree?\n\nSee where you are:\n\n```bash\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh list\n```\n\n### .env files missing in worktree?\n\nIf a worktree was created without .env files (e.g., via raw `git worktree add`), copy them:\n\n```bash\nbash ${CLAUDE_PLUGIN_ROOT}/skills/git-worktree/scripts/worktree-manager.sh copy-env feature-name\n```\n\nNavigate back to main:\n\n```bash\ncd $(git rev-parse --show-toplevel)\n```\n\n## Technical Details\n\n### Directory Structure\n\n```\n.worktrees/\n├── feature-login/          # Worktree 1\n│   ├── .git\n│   ├── app/\n│   └── ...\n├── feature-notifications/  # Worktree 2\n│   ├── .git\n│   ├── app/\n│   └── ...\n└── ...\n\n.gitignore (updated to include .worktrees)\n```\n\n### How It Works\n\n- Uses `git worktree add` for isolated environments\n- Each worktree has its own branch\n- Changes in one worktree don't affect others\n- Share git history with main repo\n- Can push from any worktree\n\n### Performance\n\n- Worktrees are lightweight (just file system links)\n- No repository duplication\n- Shared git objects for efficiency\n- Much faster than cloning or stashing/switching"
  },
  {
    "name": "learnings",
    "description": "Capture solved problems as categorized documentation with YAML frontmatter for fast lookup",
    "template": "# learnings Skill\n\n**Purpose:** Automatically document solved problems to build searchable institutional knowledge with category-based organization (enum-validated problem types).\n\n## Overview\n\nThis skill captures problem solutions immediately after confirmation, creating structured documentation that serves as a searchable knowledge base for future sessions.\n\n**Organization:** Single-file architecture - each problem documented as one markdown file in its symptom category directory (e.g., `docs/solutions/performance-issues/n-plus-one-briefs.md`). Files use YAML frontmatter for metadata and searchability.\n\n---\n\n<critical_sequence name=\"documentation-capture\" enforce_order=\"strict\">\n\n## 7-Step Process\n\n<step number=\"1\" required=\"true\">\n### Step 1: Detect Confirmation\n\n**Auto-invoke after phrases:**\n\n- \"that worked\"\n- \"it's fixed\"\n- \"working now\"\n- \"problem solved\"\n- \"that did it\"\n\n**OR manual:** `/doc-fix` command\n\n**Non-trivial problems only:**\n\n- Multiple investigation attempts needed\n- Tricky debugging that took time\n- Non-obvious solution\n- Future sessions would benefit\n\n**Skip documentation for:**\n\n- Simple typos\n- Obvious syntax errors\n- Trivial fixes immediately corrected\n  </step>\n\n<step number=\"2\" required=\"true\" depends_on=\"1\">\n### Step 2: Gather Context\n\nExtract from conversation history:\n\n**Required information:**\n\n- **Module name**: Which module or component had the problem\n- **Symptom**: Observable error/behavior (exact error messages)\n- **Investigation attempts**: What didn't work and why\n- **Root cause**: Technical explanation of actual problem\n- **Solution**: What fixed it (code/config changes)\n- **Prevention**: How to avoid in future\n\n**Environment details:**\n\n- Rails version\n- Stage (0-6 or post-implementation)\n- OS version\n- File/line references\n\n**BLOCKING REQUIREMENT:** If critical context is missing (module name, exact error, stage, or resolution steps), ask user and WAIT for response before proceeding to Step 3:\n\n```\nI need a few details to document this properly:\n\n1. Which module had this issue? [ModuleName]\n2. What was the exact error message or symptom?\n3. What stage were you in? (0-6 or post-implementation)\n\n[Continue after user provides details]\n```\n\n</step>\n\n<step number=\"3\" required=\"false\" depends_on=\"2\">\n### Step 3: Check Existing Docs\n\nSearch docs/solutions/ for similar issues:\n\n```bash\n# Search by error message keywords\ngrep -r \"exact error phrase\" docs/solutions/\n\n# Search by symptom category\nls docs/solutions/[category]/\n```\n\n**IF similar issue found:**\n\nTHEN present decision options:\n\n```\nFound similar issue: docs/solutions/[path]\n\nWhat's next?\n1. Create new doc with cross-reference (recommended)\n2. Update existing doc (only if same root cause)\n3. Other\n\nChoose (1-3): _\n```\n\nWAIT for user response, then execute chosen action.\n\n**ELSE** (no similar issue found):\n\nProceed directly to Step 4 (no user interaction needed).\n</step>\n\n<step number=\"4\" required=\"true\" depends_on=\"2\">\n### Step 4: Generate Filename\n\nFormat: `[sanitized-symptom]-[module]-[YYYYMMDD].md`\n\n**Sanitization rules:**\n\n- Lowercase\n- Replace spaces with hyphens\n- Remove special characters except hyphens\n- Truncate to reasonable length (< 80 chars)\n\n**Examples:**\n\n- `missing-include-BriefSystem-20251110.md`\n- `parameter-not-saving-state-EmailProcessing-20251110.md`\n- `webview-crash-on-resize-Assistant-20251110.md`\n  </step>\n\n<step number=\"5\" required=\"true\" depends_on=\"4\" blocking=\"true\">\n### Step 5: Validate YAML Schema\n\n**CRITICAL:** All docs require validated YAML frontmatter with enum validation.\n\n<validation_gate name=\"yaml-schema\" blocking=\"true\">\n\n**Validate against schema:**\nLoad `schema.yaml` and classify the problem against the enum values defined in [yaml-schema.md](./references/yaml-schema.md). Ensure all required fields are present and match allowed values exactly.\n\n**BLOCK if validation fails:**\n\n```\n❌ YAML validation failed\n\nErrors:\n- problem_type: must be one of schema enums, got \"compilation_error\"\n- severity: must be one of [critical, high, medium, low], got \"invalid\"\n- symptoms: must be array with 1-5 items, got string\n\nPlease provide corrected values.\n```\n\n**GATE ENFORCEMENT:** Do NOT proceed to Step 6 (Create Documentation) until YAML frontmatter passes all validation rules defined in `schema.yaml`.\n\n</validation_gate>\n</step>\n\n<step number=\"6\" required=\"true\" depends_on=\"5\">\n### Step 6: Create Documentation\n\n**Determine category from problem_type:** Use the category mapping defined in [yaml-schema.md](./references/yaml-schema.md) (lines 49-61).\n\n**Create documentation file:**\n\n```bash\nPROBLEM_TYPE=\"[from validated YAML]\"\nCATEGORY=\"[mapped from problem_type]\"\nFILENAME=\"[generated-filename].md\"\nDOC_PATH=\"docs/solutions/${CATEGORY}/${FILENAME}\"\n\n# Create directory if needed\nmkdir -p \"docs/solutions/${CATEGORY}\"\n\n# Write documentation using template from assets/resolution-template.md\n# (Content populated with Step 2 context and validated YAML frontmatter)\n```\n\n**Result:**\n\n- Single file in category directory\n- Enum validation ensures consistent categorization\n\n**Create documentation:** Populate the structure from `assets/resolution-template.md` with context gathered in Step 2 and validated YAML frontmatter from Step 5.\n</step>\n\n<step number=\"7\" required=\"false\" depends_on=\"6\">\n### Step 7: Cross-Reference & Critical Pattern Detection\n\nIf similar issues found in Step 3:\n\n**Update existing doc:**\n\n```bash\n# Add Related Issues link to similar doc\necho \"- See also: [$FILENAME]($REAL_FILE)\" >> [similar-doc.md]\n```\n\n**Update new doc:**\nAlready includes cross-reference from Step 6.\n\n**Update patterns if applicable:**\n\nIf this represents a common pattern (3+ similar issues):\n\n```bash\n# Add to docs/solutions/patterns/common-solutions.md\ncat >> docs/solutions/patterns/common-solutions.md << 'EOF'\n\n## [Pattern Name]\n\n**Common symptom:** [Description]\n**Root cause:** [Technical explanation]\n**Solution pattern:** [General approach]\n\n**Examples:**\n- [Link to doc 1]\n- [Link to doc 2]\n- [Link to doc 3]\nEOF\n```\n\n**Critical Pattern Detection (Optional Proactive Suggestion):**\n\nIf this issue has automatic indicators suggesting it might be critical:\n\n- Severity: `critical` in YAML\n- Affects multiple modules OR foundational stage (Stage 2 or 3)\n- Non-obvious solution\n\nThen in the decision menu (Step 8), add a note:\n\n```\n💡 This might be worth adding to Required Reading (Option 2)\n```\n\nBut **NEVER auto-promote**. User decides via decision menu (Option 2).\n\n**Template for critical pattern addition:**\n\nWhen user selects Option 2 (Add to Required Reading), use the template from `assets/critical-pattern-template.md` to structure the pattern entry. Number it sequentially based on existing patterns in `docs/solutions/patterns/critical-patterns.md`.\n</step>\n\n</critical_sequence>\n\n---\n\n<decision_gate name=\"post-documentation\" wait_for_user=\"true\">\n\n## Decision Menu After Capture\n\nAfter successful documentation, present options and WAIT for user response:\n\n```\n✓ Solution documented\n\nFile created:\n- docs/solutions/[category]/[filename].md\n\nWhat's next?\n1. Continue workflow (recommended)\n2. Add to Required Reading - Promote to critical patterns (critical-patterns.md)\n3. Link related issues - Connect to similar problems\n4. Add to existing skill - Add to a learning skill (e.g., hotwire-native)\n5. Create new skill - Extract into new learning skill\n6. View documentation - See what was captured\n7. Other\n```\n\n**Handle responses:**\n\n**Option 1: Continue workflow**\n\n- Return to calling skill/workflow\n- Documentation is complete\n\n**Option 2: Add to Required Reading** ⭐ PRIMARY PATH FOR CRITICAL PATTERNS\n\nUser selects this when:\n\n- System made this mistake multiple times across different modules\n- Solution is non-obvious but must be followed every time\n- Foundational requirement (Rails, Rails API, threading, etc.)\n\nAction:\n\n1. Extract pattern from the documentation\n2. Format as ❌ WRONG vs ✅ CORRECT with code examples\n3. Add to `docs/solutions/patterns/critical-patterns.md`\n4. Add cross-reference back to this doc\n5. Confirm: \"✓ Added to Required Reading. All subagents will see this pattern before code generation.\"\n\n**Option 3: Link related issues**\n\n- Prompt: \"Which doc to link? (provide filename or describe)\"\n- Search docs/solutions/ for the doc\n- Add cross-reference to both docs\n- Confirm: \"✓ Cross-reference added\"\n\n**Option 4: Add to existing skill**\n\nUser selects this when the documented solution relates to an existing learning skill:\n\nAction:\n\n1. Prompt: \"Which skill? (hotwire-native, etc.)\"\n2. Determine which reference file to update (resources.md, patterns.md, or examples.md)\n3. Add link and brief description to appropriate section\n4. Confirm: \"✓ Added to [skill-name] skill in [file]\"\n\nExample: For Hotwire Native Tailwind variants solution:\n\n- Add to `hotwire-native/references/resources.md` under \"Project-Specific Resources\"\n- Add to `hotwire-native/references/examples.md` with link to solution doc\n\n**Option 5: Create new skill**\n\nUser selects this when the solution represents the start of a new learning domain:\n\nAction:\n\n1. Prompt: \"What should the new skill be called? (e.g., stripe-billing, email-processing)\"\n2. Run `python3 .claude/skills/skill-creator/scripts/init_skill.py [skill-name]`\n3. Create initial reference files with this solution as first example\n4. Confirm: \"✓ Created new [skill-name] skill with this solution as first example\"\n\n**Option 6: View documentation**\n\n- Display the created documentation\n- Present decision menu again\n\n**Option 7: Other**\n\n- Ask what they'd like to do\n\n</decision_gate>\n\n---\n\n<integration_protocol>\n\n## Integration Points\n\n**Invoked by:**\n\n- /learnings command (primary interface)\n- Manual invocation in conversation after solution confirmed\n- Can be triggered by detecting confirmation phrases like \"that worked\", \"it's fixed\", etc.\n\n**Invokes:**\n\n- None (terminal skill - does not delegate to other skills)\n\n**Handoff expectations:**\nAll context needed for documentation should be present in conversation history before invocation.\n\n</integration_protocol>\n\n---\n\n<success_criteria>\n\n## Success Criteria\n\nDocumentation is successful when ALL of the following are true:\n\n- ✅ YAML frontmatter validated (all required fields, correct formats)\n- ✅ File created in docs/solutions/[category]/[filename].md\n- ✅ Enum values match schema.yaml exactly\n- ✅ Code examples included in solution section\n- ✅ Cross-references added if related issues found\n- ✅ User presented with decision menu and action confirmed\n\n</success_criteria>\n\n---\n\n## Error Handling\n\n**Missing context:**\n\n- Ask user for missing details\n- Don't proceed until critical info provided\n\n**YAML validation failure:**\n\n- Show specific errors\n- Present retry with corrected values\n- BLOCK until valid\n\n**Similar issue ambiguity:**\n\n- Present multiple matches\n- Let user choose: new doc, update existing, or link as duplicate\n\n**Module not in modules documentation:**\n\n- Warn but don't block\n- Proceed with documentation\n- Suggest: \"Add [Module] to modules documentation if not there\"\n\n---\n\n## Execution Guidelines\n\n**MUST do:**\n\n- Validate YAML frontmatter (BLOCK if invalid per Step 5 validation gate)\n- Extract exact error messages from conversation\n- Include code examples in solution section\n- Create directories before writing files (`mkdir -p`)\n- Ask user and WAIT if critical context missing\n\n**MUST NOT do:**\n\n- Skip YAML validation (validation gate is blocking)\n- Use vague descriptions (not searchable)\n- Omit code examples or cross-references\n\n---\n\n## Quality Guidelines\n\n**Good documentation has:**\n\n- ✅ Exact error messages (copy-paste from output)\n- ✅ Specific file:line references\n- ✅ Observable symptoms (what you saw, not interpretations)\n- ✅ Failed attempts documented (helps avoid wrong paths)\n- ✅ Technical explanation (not just \"what\" but \"why\")\n- ✅ Code examples (before/after if applicable)\n- ✅ Prevention guidance (how to catch early)\n- ✅ Cross-references (related issues)\n\n**Avoid:**\n\n- ❌ Vague descriptions (\"something was wrong\")\n- ❌ Missing technical details (\"fixed the code\")\n- ❌ No context (which version? which file?)\n- ❌ Just code dumps (explain why it works)\n- ❌ No prevention guidance\n- ❌ No cross-references\n\n---\n\n## Example Scenario\n\n**User:** \"That worked! The N+1 query is fixed.\"\n\n**Skill activates:**\n\n1. **Detect confirmation:** \"That worked!\" triggers auto-invoke\n2. **Gather context:**\n   - Module: Brief System\n   - Symptom: Brief generation taking >5 seconds, N+1 query when loading email threads\n   - Failed attempts: Added pagination (didn't help), checked background job performance\n   - Solution: Added eager loading with `includes(:emails)` on Brief model\n   - Root cause: Missing eager loading causing separate database query per email thread\n3. **Check existing:** No similar issue found\n4. **Generate filename:** `n-plus-one-brief-generation-BriefSystem-20251110.md`\n5. **Validate YAML:**\n   ```yaml\n   module: Brief System\n   date: 2025-11-10\n   problem_type: performance_issue\n   component: rails_model\n   symptoms:\n     - \"N+1 query when loading email threads\"\n     - \"Brief generation taking >5 seconds\"\n   root_cause: missing_include\n   severity: high\n   tags: [n-plus-one, eager-loading, performance]\n   ```\n   ✅ Valid\n6. **Create documentation:**\n   - `docs/solutions/performance-issues/n-plus-one-brief-generation-BriefSystem-20251110.md`\n7. **Cross-reference:** None needed (no similar issues)\n\n**Output:**\n\n```\n✓ Solution documented\n\nFile created:\n- docs/solutions/performance-issues/n-plus-one-brief-generation-BriefSystem-20251110.md\n\nWhat's next?\n1. Continue workflow (recommended)\n2. Add to Required Reading - Promote to critical patterns (critical-patterns.md)\n3. Link related issues - Connect to similar problems\n4. Add to existing skill - Add to a learning skill (e.g., hotwire-native)\n5. Create new skill - Extract into new learning skill\n6. View documentation - See what was captured\n7. Other\n```\n\n---\n\n## Future Enhancements\n\n**Not in Phase 7 scope, but potential:**\n\n- Search by date range\n- Filter by severity\n- Tag-based search interface\n- Metrics (most common issues, resolution time)\n- Export to shareable format (community knowledge sharing)\n- Import community solutions"
  },
  {
    "name": "playwright",
    "description": "MUST USE for any browser-related tasks. Browser automation via Playwright MCP - verification, browsing, information gathering, web scraping, testing, screenshots, and all browser interactions.",
    "template": "# Playwright Browser Automation\n\nThis skill provides browser automation capabilities via the Playwright MCP server.",
    "mcpConfig": {
      "playwright": {
        "command": "npx",
        "args": [
          "@playwright/mcp@latest"
        ]
      }
    }
  },
  {
    "name": "rclone",
    "description": "Upload, sync, and manage files across cloud storage providers using rclone. Use when uploading files (images, videos, documents) to S3, Cloudflare R2, Backblaze B2, Google Drive, Dropbox, or any S3-compatible storage. Triggers on \"upload to S3\", \"sync to cloud\", \"rclone\", \"backup files\", \"upload video/image to bucket\", or requests to transfer files to remote storage.",
    "template": "# rclone File Transfer Skill\n\n## Setup Check (Always Run First)\n\nBefore any rclone operation, verify installation and configuration:\n\n```bash\n# Check if rclone is installed\ncommand -v rclone >/dev/null 2>&1 && echo \"rclone installed: $(rclone version | head -1)\" || echo \"NOT INSTALLED\"\n\n# List configured remotes\nrclone listremotes 2>/dev/null || echo \"NO REMOTES CONFIGURED\"\n```\n\n### If rclone is NOT installed\n\nGuide the user to install:\n\n```bash\n# macOS\nbrew install rclone\n\n# Linux (script install)\ncurl https://rclone.org/install.sh | sudo bash\n\n# Or via package manager\nsudo apt install rclone  # Debian/Ubuntu\nsudo dnf install rclone  # Fedora\n```\n\n### If NO remotes are configured\n\nWalk the user through interactive configuration:\n\n```bash\nrclone config\n```\n\n**Common provider setup quick reference:**\n\n| Provider            | Type      | Key Settings                                                                     |\n| ------------------- | --------- | -------------------------------------------------------------------------------- |\n| AWS S3              | `s3`      | access_key_id, secret_access_key, region                                         |\n| Cloudflare R2       | `s3`      | access_key_id, secret_access_key, endpoint (account_id.r2.cloudflarestorage.com) |\n| Backblaze B2        | `b2`      | account (keyID), key (applicationKey)                                            |\n| DigitalOcean Spaces | `s3`      | access_key_id, secret_access_key, endpoint (region.digitaloceanspaces.com)       |\n| Google Drive        | `drive`   | OAuth flow (opens browser)                                                       |\n| Dropbox             | `dropbox` | OAuth flow (opens browser)                                                       |\n\n**Example: Configure Cloudflare R2**\n\n```bash\nrclone config create r2 s3 \\\n  provider=Cloudflare \\\n  access_key_id=YOUR_ACCESS_KEY \\\n  secret_access_key=YOUR_SECRET_KEY \\\n  endpoint=ACCOUNT_ID.r2.cloudflarestorage.com \\\n  acl=private\n```\n\n**Example: Configure AWS S3**\n\n```bash\nrclone config create aws s3 \\\n  provider=AWS \\\n  access_key_id=YOUR_ACCESS_KEY \\\n  secret_access_key=YOUR_SECRET_KEY \\\n  region=us-east-1\n```\n\n## Common Operations\n\n### Upload single file\n\n```bash\nrclone copy /path/to/file.mp4 remote:bucket/path/ --progress\n```\n\n### Upload directory\n\n```bash\nrclone copy /path/to/folder remote:bucket/folder/ --progress\n```\n\n### Sync directory (mirror, deletes removed files)\n\n```bash\nrclone sync /local/path remote:bucket/path/ --progress\n```\n\n### List remote contents\n\n```bash\nrclone ls remote:bucket/\nrclone lsd remote:bucket/  # directories only\n```\n\n### Check what would be transferred (dry run)\n\n```bash\nrclone copy /path remote:bucket/ --dry-run\n```\n\n## Useful Flags\n\n| Flag                | Purpose                            |\n| ------------------- | ---------------------------------- |\n| `--progress`        | Show transfer progress             |\n| `--dry-run`         | Preview without transferring       |\n| `-v`                | Verbose output                     |\n| `--transfers=N`     | Parallel transfers (default 4)     |\n| `--bwlimit=RATE`    | Bandwidth limit (e.g., `10M`)      |\n| `--checksum`        | Compare by checksum, not size/time |\n| `--exclude=\"*.tmp\"` | Exclude patterns                   |\n| `--include=\"*.mp4\"` | Include only matching              |\n| `--min-size=SIZE`   | Skip files smaller than SIZE       |\n| `--max-size=SIZE`   | Skip files larger than SIZE        |\n\n## Large File Uploads\n\nFor videos and large files, use chunked uploads:\n\n```bash\n# S3 multipart upload (automatic for >200MB)\nrclone copy large_video.mp4 remote:bucket/ --s3-chunk-size=64M --progress\n\n# Resume interrupted transfers\nrclone copy /path remote:bucket/ --progress --retries=5\n```\n\n## Verify Upload\n\n```bash\n# Check file exists and matches\nrclone check /local/file remote:bucket/file\n\n# Get file info\nrclone lsl remote:bucket/path/to/file\n```\n\n## Troubleshooting\n\n```bash\n# Test connection\nrclone lsd remote:\n\n# Debug connection issues\nrclone lsd remote: -vv\n\n# Check config\nrclone config show remote\n```"
  },
  {
    "name": "skill-creator",
    "description": "Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.",
    "template": "# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasks—they transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n├── SKILL.md (required)\n│   ├── YAML frontmatter metadata (required)\n│   │   ├── name: (required)\n│   │   └── description: (required)\n│   └── Markdown instructions (required)\n└── Bundled Resources (optional)\n    ├── scripts/          - Executable code (Python/Bash/etc.)\n    ├── references/       - Documentation intended to be loaded into context as needed\n    └── assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\n**Metadata Quality:** The `name` and `description` in YAML frontmatter determine when Claude will use the skill. Be specific about what the skill does and when to use it. Use the third-person (e.g. \"This skill should be used when...\" instead of \"Use this skill when...\").\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skill—this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited\\*)\n\n\\*Unlimited because scripts can be executed without reading into context window.\n\n## Skill Creation Process\n\nTo create a skill, follow the \"Skill Creation Process\" in order, skipping steps only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Focus on including information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAlso, delete any example files and directories not needed for the skill. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Style:** Write the entire skill using **imperative/infinitive form** (verb-first instructions), not second person. Use objective, instructional language (e.g., \"To accomplish X, do Y\" rather than \"You should do X\" or \"If you need to do X\"). This maintains consistency and clarity for AI consumption.\n\nTo complete SKILL.md, answer the following questions:\n\n1. What is the purpose of the skill, in a few sentences?\n2. When should the skill be used?\n3. In practice, how should Claude use the skill? All reusable skill contents developed above should be referenced so that Claude knows how to use them.\n\n### Step 5: Packaging a Skill\n\nOnce the skill is ready, it should be packaged into a distributable zip file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a zip file named after the skill (e.g., `my-skill.zip`) that includes all files and maintains the proper directory structure for distribution.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again"
  },
  {
    "name": "ultrawork-loop",
    "description": "Skill docs for the `ultrawork-loop` drain agent. Documents expected sequence, flags, and completion-handshake semantics.",
    "template": "# Ultrawork Loop Skill\n\n## Overview\n\n`ultrawork-loop` is a small orchestration/drain agent used by the harness to ensure work items (PR fixes and file-based todos) are completed before continuing downstream workflows. It runs a predefined, sequential set of steps and emits a completion promise (`DONE`) when finished.\n\n## When to use\n\n- You need a deterministic \"drain\" step at the end of a workflow\n- You want the harness to wait until PR fixes and todos are resolved\n- You want an explicit completion handshake (`--completion-promise \"DONE\"`)\n\n## Sequence (fixed)\n\n1. `/ghostwire:resolve_pr_parallel`\n2. `/ghostwire:resolve_todo_parallel --completion-promise \"DONE\"`\n\nThe second step must include `--completion-promise \"DONE\"` so the harness recognizes completion.\n\n## Expected Output\n\n- On success: the agent or invoked command outputs exactly:\n\n  <promise>DONE</promise>\n\n- On partial failure: surface failing items as todos or PR comments and exit with a non-success status (human follow-up required)\n\n## Implementation notes for integrators\n\n- Keep `ultrawork-loop` narrow and predictable. It should not spawn dynamic plans or long-running creative tasks.\n- Preserve the `--completion-promise` semantics: consumers depend on the exact token `DONE` (case-sensitive) wrapped in `<promise>` tags.\n\n## Testing\n\n- Unit test should verify:\n  - `agents/ultrawork-loop.md` exists and documents the sequence\n  - `skills/ultrawork-loop/SKILL.md` contains the completion-promise guidance\n  - `opencode.json` (or the calling workflow) includes the sequential resolvers and the `--completion-promise \"DONE\"` flag where expected\n\n## Example invocation\n\n- Synchronous run (from workflow):\n  1. Start `ultrawork-loop`\n  2. Wait for `<promise>DONE</promise>`\n  3. Proceed to `workflows:plan` / `workflows:work` etc.\n\n## Troubleshooting\n\n- If the harness never receives `DONE`, inspect logs from `/ghostwire:resolve_todo_parallel` for blocked todos or failing PRs.\n- Ensure the `--completion-promise` flag was preserved when invoking the TODO resolver.\n\n## Compatibility\n\n- Designed to be used in the existing `lfg` workflow and any other workflows that require a deterministic drain step.\n\n---\n\n`ultrawork-loop` is intentionally small: orchestration only, no guessing or planning."
  }
] as const;

export const SKILL_NAME_VALUES = [
  "agent-browser",
  "andrew-kane-gem-writer",
  "brainstorming",
  "coding-tutor",
  "creating-agent-skills",
  "dev-browser",
  "dhh-rails-style",
  "dspy-ruby",
  "every-style-editor",
  "file-todos",
  "frontend-design",
  "frontend-ui-ux",
  "gemini-imagegen",
  "git-master",
  "git-worktree",
  "learnings",
  "playwright",
  "rclone",
  "skill-creator",
  "ultrawork-loop"
] as const;

export type SkillName = (typeof SKILL_NAME_VALUES)[number];

export const SKILLS_MANIFEST_RESOLUTION = {
  canonicalPath: ".agents/skills",
  collisionPolicy: "first-wins",
  builtinMerge: "fallback-after-scoped",
} as const;

export function mergeBuiltinManifestAfterScoped(scopedSkills: ReadonlyArray<Skill>): Skill[] {
  const mergedSkills: Skill[] = [...scopedSkills];
  const seenNames = new Set(scopedSkills.map((skill) => skill.name));

  for (const builtinSkill of SKILLS_MANIFEST) {
    if (seenNames.has(builtinSkill.name)) {
      continue;
    }
    seenNames.add(builtinSkill.name);
    mergedSkills.push(builtinSkill);
  }

  return mergedSkills;
}
